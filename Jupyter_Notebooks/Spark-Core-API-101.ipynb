{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**parallelize** \n",
    "\n",
    "parallelizing in PySpark means splitting a task or dataset into smaller parts and processing them simultaneously on multiple machines or cores to speed up computation.\n",
    "\n",
    "In PySpark, this is done using the `parallelize()` method, which takes a regular Python collection (like a list or a set) and converts it into an RDD (Resilient Distributed Dataset). This RDD can then be processed in parallel across a cluster of machines.\n",
    "\n",
    "For example, if you have a large dataset (like a list of numbers), instead of processing the entire dataset sequentially, PySpark will break it into smaller chunks and work on them in parallel. This results in faster processing, especially when dealing with large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/20 13:57:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/12/20 13:57:15 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySparkExample\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = (\"big\",\"Data\",\"Is\",\"SUPER\",\"Interesting\",\"BIG\",\"data\",\"IS\",\"A\",\"Trending\",\"technology\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_rdd = spark.sparkContext.parallelize(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_normalized = words_rdd.map(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['big',\n",
       " 'data',\n",
       " 'is',\n",
       " 'super',\n",
       " 'interesting',\n",
       " 'big',\n",
       " 'data',\n",
       " 'is',\n",
       " 'a',\n",
       " 'trending',\n",
       " 'technology']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_normalized.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_words = words_normalized.map(lambda x:(x,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_result = mapped_words.reduceByKey(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('interesting', 1),\n",
       " ('trending', 1),\n",
       " ('technology', 1),\n",
       " ('a', 1),\n",
       " ('data', 2),\n",
       " ('super', 1),\n",
       " ('is', 2),\n",
       " ('big', 2)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregated_result.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chaining functions together in PySpark** \n",
    "\n",
    "Which means applying multiple operations or transformations one after another to an RDD or DataFrame without having to assign intermediate results to separate variables.\n",
    "\n",
    "In PySpark, operations like `map()`, `filter()`, `reduce()`, or `select()` can be chained together to perform a series of transformations or actions on your data. Chaining makes the code more concise and readable, and it allows Spark to optimize the execution plan by combining these operations into a single pipeline.\n",
    "\n",
    "By chaining the .map(), .filter(), and .collect() methods together with the `\\ (backslash)`, we avoid creating intermediate variables and directly apply the transformations in sequence.\n",
    "\n",
    "**Why Chaining is Useful**\n",
    "\n",
    "*   **Conciseness:** It reduces the need for intermediate variables and makes your code more compact.\n",
    "*   **Readability:** It makes it clear that a series of transformations are being applied to the data\n",
    "*   **Optimization:** Spark can optimize the sequence of operations in a single execution plan, potentially making the computation more efficient.\n",
    "\n",
    "In summary, chaining in PySpark allows you to apply multiple transformations and actions in sequence without intermediate steps, which results in cleaner, more efficient code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = spark.sparkContext.parallelize(words)  \\\n",
    ".map(lambda x: x.lower())   \\\n",
    ".map(lambda x:(x,1))    \\\n",
    ".reduceByKey(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('interesting', 1),\n",
       " ('trending', 1),\n",
       " ('technology', 1),\n",
       " ('a', 1),\n",
       " ('data', 2),\n",
       " ('super', 1),\n",
       " ('is', 2),\n",
       " ('big', 2)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How will you check that how many partitions your rdd has?**\n",
    "\n",
    "To check how many partitions an RDD has in PySpark, you can use the `getNumPartitions()` method.\n",
    "\n",
    "This method returns the number of partitions in the RDD, which is useful for understanding how the data is distributed across the Spark cluster.\n",
    "\n",
    "\n",
    "**Why is this useful?**\n",
    "\n",
    "*   Knowing the number of partitions is helpful for optimizing performance in Spark. Too few partitions may lead to uneven workload distribution, while too many partitions could lead to overhead in managing them.\n",
    "\n",
    "*   You can also repartition your RDD to control the number of partitions using operations like     `repartition()` or `coalesce()` to optimize the performance based on your cluster's configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value returned by `getNumPartitions()` in PySpark depends on several factors, such as how the RDD was created and how the data is distributed across the cluster or local machine. The number of partitions is influenced by the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Number of Partitions Specified During RDD Creation**\n",
    "\n",
    "When you create an RDD, you can specify the number of partitions explicitly. For example, using sc.`parallelize()`, you can set the number of partitions to control how the data will be distributed.\n",
    "\n",
    "Example:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 3)\n",
    "print(rdd.getNumPartitions())  # Output: 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, you specified 3 partitions, so the result will be 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Default Number of Partitions (When Not Specified)**\n",
    "\n",
    "\n",
    "If you don't specify the number of partitions during RDD creation (e.g., using sc.parallelize()), Spark uses the default number of partitions, which is determined by:\n",
    "\n",
    "*   **For Local Mode:** The default number of partitions is typically the number of available cores on the machine.\n",
    "    *   Example: If you run Spark on a machine with 8 cores, the default parallelism will be 8, and sc.parallelize(data) will create an RDD with 8 partitions by default.\n",
    "\n",
    "*   **For Cluster Mode:** The default number of partitions will be based on the total number of cores across all nodes in the cluster.\n",
    "    *   Example: If you have 10 nodes, each with 8 cores, the default parallelism might be 80.\n",
    "\n",
    "This default number of partitions can be retrieved using `spark.sparkContext.defaultParallelism`.\n",
    "\n",
    "Example:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(spark.sparkContext.defaultParallelism)  # Output might be 8 (if running locally with 8 cores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.Data Source and File Size**\n",
    "\n",
    "When you're reading data from an external source (e.g., HDFS, S3, or local files), the number of partitions can be influenced by:\n",
    "\n",
    "*   The number of input files and their size.\n",
    "\n",
    "*   The default partitioning behavior of Spark when reading the data (e.g., how it splits files into partitions).\n",
    "\n",
    "For example:\n",
    "\n",
    "*   If you load a large text file, Spark will split the file into partitions based on block size (typically 128 MB per partition in HDFS).\n",
    "*   If you load the small, Spark may split create a partition & distribute the data based on spark.`sparkContext.defaultMinPartitions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.defaultMinPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_rdd = spark.sparkContext.textFile(\"/Users/sugumarsrinivasan/Documents/data/input_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" if the file size is less than 128 MB(i.e. default block size in HDFS), \n",
    "then Number of partitions are equal to spark.sparkContext.deafultMinPartitions set in the cluster.\"\"\"\n",
    "base_rdd.getNumPartitions() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pyspark-env)",
   "language": "python",
   "name": "pyspark-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
