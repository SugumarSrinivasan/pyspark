{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comprehensive Guide to Creating DataFrames in PySpark:**\n",
    "\n",
    "In PySpark, there are several ways to create a DataFrame. Below are the most common methods:\n",
    "\n",
    "-   spark.read()\n",
    "    -  From CSV Files   (using  spark.read.csv())\n",
    "    -  From JSON Files  (using spark.read.json())\n",
    "    -  From Parquet Files   (using spark.read.parquet())\n",
    "    -  From orc Files   (using spark.read.orc())\n",
    "\n",
    "- spark.sql()\n",
    "\n",
    "-   spark.table(\"db_schema.tbl_name\")\n",
    "\n",
    "-   spark.createDataFrame()\n",
    "\n",
    "-   spark.range()\n",
    "\n",
    "-   spark rdd\n",
    "    -   rdd.toDF(list of column names)\n",
    "    -   rdd.toDF(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/27 21:19:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init\n",
    "import getpass\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "username = getpass.getuser()\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config(\"spark.sql.catalogImplementation\", \"hive\"). \\\n",
    "    config(\"spark.sql.warehouse.dir\",f\"/Users/{username}/Documents/data/warehouse\"). \\\n",
    "    enableHiveSupport(). \\\n",
    "    master(\"local\"). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "|   retail|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "|         |   orders|       true|\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------+------------+\n",
      "|order_id|         order_date|customer_id|order_status|\n",
      "+--------+-------------------+-----------+------------+\n",
      "|       1|2013-07-27 00:00:00|      30265|      CLOSED|\n",
      "|       2|2013-11-25 00:00:00|      20386|      CLOSED|\n",
      "|       6|2014-07-20 00:00:00|      49340|      CLOSED|\n",
      "|       9|2014-01-07 00:00:00|      26329|      CLOSED|\n",
      "|      15|2014-06-10 00:00:00|       4869|      CLOSED|\n",
      "|      19|2013-09-10 00:00:00|      46034|      CLOSED|\n",
      "|      26|2014-07-19 00:00:00|      43824|      CLOSED|\n",
      "|      27|2014-02-20 00:00:00|      11361|      CLOSED|\n",
      "|      29|2013-08-22 00:00:00|      32688|      CLOSED|\n",
      "|      30|2013-08-29 00:00:00|      17044|      CLOSED|\n",
      "+--------+-------------------+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from orders where order_status = 'CLOSED' limit 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = spark.sql(\"select * from orders where order_status = 'CLOSED' limit 10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------+------------+\n",
      "|order_id|         order_date|customer_id|order_status|\n",
      "+--------+-------------------+-----------+------------+\n",
      "|       1|2013-07-27 00:00:00|      30265|      CLOSED|\n",
      "|       2|2013-11-25 00:00:00|      20386|      CLOSED|\n",
      "|       6|2014-07-20 00:00:00|      49340|      CLOSED|\n",
      "|       9|2014-01-07 00:00:00|      26329|      CLOSED|\n",
      "|      15|2014-06-10 00:00:00|       4869|      CLOSED|\n",
      "|      19|2013-09-10 00:00:00|      46034|      CLOSED|\n",
      "|      26|2014-07-19 00:00:00|      43824|      CLOSED|\n",
      "|      27|2014-02-20 00:00:00|      11361|      CLOSED|\n",
      "|      29|2013-08-22 00:00:00|      32688|      CLOSED|\n",
      "|      30|2013-08-29 00:00:00|      17044|      CLOSED|\n",
      "+--------+-------------------+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+-----------+\n",
      "|namespace|     tableName|isTemporary|\n",
      "+---------+--------------+-----------+\n",
      "|  default|orders_managed|      false|\n",
      "|         |        orders|       true|\n",
      "+---------+--------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = spark.table(\"spark_catalog.default.orders_managed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------+------------+\n",
      "|order_id|         order_date|customer_id|order_status|\n",
      "+--------+-------------------+-----------+------------+\n",
      "|       1|2013-07-27 00:00:00|      30265|      CLOSED|\n",
      "|       2|2013-11-25 00:00:00|      20386|      CLOSED|\n",
      "|       3|2014-01-21 00:00:00|      15768|    COMPLETE|\n",
      "|       4|2014-07-04 00:00:00|      27181|  PROCESSING|\n",
      "|       5|2014-03-08 00:00:00|      12448|    COMPLETE|\n",
      "+--------+-------------------+-----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_list = [\n",
    "(1,'2013-07-27 00:00:00.0',30265,'CLOSED'),\n",
    "(2,'2013-11-25 00:00:00.0',20386,'CLOSED'),\n",
    "(3,'2014-01-21 00:00:00.0',15768,'COMPLETE')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = spark.createDataFrame(orders_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----+--------+\n",
      "| _1|                  _2|   _3|      _4|\n",
      "+---+--------------------+-----+--------+\n",
      "|  1|2013-07-27 00:00:...|30265|  CLOSED|\n",
      "|  2|2013-11-25 00:00:...|20386|  CLOSED|\n",
      "|  3|2014-01-21 00:00:...|15768|COMPLETE|\n",
      "+---+--------------------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: long (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: long (nullable = true)\n",
      " |-- _4: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = spark.createDataFrame(orders_list).toDF('order_id','order_date','customer_id','order_status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------+------------+\n",
      "|order_id|          order_date|customer_id|order_status|\n",
      "+--------+--------------------+-----------+------------+\n",
      "|       1|2013-07-27 00:00:...|      30265|      CLOSED|\n",
      "|       2|2013-11-25 00:00:...|      20386|      CLOSED|\n",
      "|       3|2014-01-21 00:00:...|      15768|    COMPLETE|\n",
      "+--------+--------------------+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: long (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- customer_id: long (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_schema = [\"order_id\",\"order_date\",\"customer_id\",\"order_status\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = spark.createDataFrame(orders_list,orders_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------+------------+\n",
      "|order_id|          order_date|customer_id|order_status|\n",
      "+--------+--------------------+-----------+------------+\n",
      "|       1|2013-07-27 00:00:...|      30265|      CLOSED|\n",
      "|       2|2013-11-25 00:00:...|      20386|      CLOSED|\n",
      "|       3|2014-01-21 00:00:...|      15768|    COMPLETE|\n",
      "+--------+--------------------+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: long (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- customer_id: long (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_schema = 'orderid integer, orderdate string, customerid integer, orderstatus string'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = spark.createDataFrame(orders_list,orders_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+----------+-----------+\n",
      "|orderid|           orderdate|customerid|orderstatus|\n",
      "+-------+--------------------+----------+-----------+\n",
      "|      1|2013-07-27 00:00:...|     30265|     CLOSED|\n",
      "|      2|2013-11-25 00:00:...|     20386|     CLOSED|\n",
      "|      3|2014-01-21 00:00:...|     15768|   COMPLETE|\n",
      "+-------+--------------------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- orderid: integer (nullable = true)\n",
      " |-- orderdate: string (nullable = true)\n",
      " |-- customerid: integer (nullable = true)\n",
      " |-- orderstatus: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = new_df.withColumn(\"orderdate\",to_timestamp('orderdate'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+----------+-----------+\n",
      "|orderid|          orderdate|customerid|orderstatus|\n",
      "+-------+-------------------+----------+-----------+\n",
      "|      1|2013-07-27 00:00:00|     30265|     CLOSED|\n",
      "|      2|2013-11-25 00:00:00|     20386|     CLOSED|\n",
      "|      3|2014-01-21 00:00:00|     15768|   COMPLETE|\n",
      "+-------+-------------------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- orderid: integer (nullable = true)\n",
      " |-- orderdate: timestamp (nullable = true)\n",
      " |-- customerid: integer (nullable = true)\n",
      " |-- orderstatus: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(0,8).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  2|\n",
      "|  4|\n",
      "|  6|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(0,8,2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  3|\n",
      "|  6|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(0,8,3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_rdd = spark.sparkContext.textFile(\"/Users/sugumarsrinivasan/Documents/data/orders.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1,2013-07-27 00:00:00.0,30265,CLOSED',\n",
       " '2,2013-11-25 00:00:00.0,20386,CLOSED',\n",
       " '3,2014-01-21 00:00:00.0,15768,COMPLETE',\n",
       " '4,2014-07-04 00:00:00.0,27181,PROCESSING',\n",
       " '5,2014-03-08 00:00:00.0,12448,COMPLETE']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_orders_rdd = orders_rdd.map(lambda x: (int(x.split(\",\")[0]), x.split(\",\")[1], int(x.split(\",\")[2]), x.split(\",\")[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1, '2013-07-27 00:00:00.0', 30265, 'CLOSED'),\n",
       " (2, '2013-11-25 00:00:00.0', 20386, 'CLOSED'),\n",
       " (3, '2014-01-21 00:00:00.0', 15768, 'COMPLETE'),\n",
       " (4, '2014-07-04 00:00:00.0', 27181, 'PROCESSING'),\n",
       " (5, '2014-03-08 00:00:00.0', 12448, 'COMPLETE')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_orders_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_schema = 'order_id integer, order_date string, customer_id long, order_status string'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(new_orders_rdd,orders_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------+------------+\n",
      "|order_id|          order_date|customer_id|order_status|\n",
      "+--------+--------------------+-----------+------------+\n",
      "|       1|2013-07-27 00:00:...|      30265|      CLOSED|\n",
      "|       2|2013-11-25 00:00:...|      20386|      CLOSED|\n",
      "|       3|2014-01-21 00:00:...|      15768|    COMPLETE|\n",
      "|       4|2014-07-04 00:00:...|      27181|  PROCESSING|\n",
      "|       5|2014-03-08 00:00:...|      12448|    COMPLETE|\n",
      "+--------+--------------------+-----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/27 21:00:01 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 20 (TID 20): Attempting to kill Python Worker\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- customer_id: long (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = new_orders_rdd.toDF(['order_id','order_date','customer_id','order_status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/27 21:11:07 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 3 (TID 3): Attempting to kill Python Worker\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------+------------+\n",
      "|order_id|          order_date|customer_id|order_status|\n",
      "+--------+--------------------+-----------+------------+\n",
      "|       1|2013-07-27 00:00:...|      30265|      CLOSED|\n",
      "|       2|2013-11-25 00:00:...|      20386|      CLOSED|\n",
      "|       3|2014-01-21 00:00:...|      15768|    COMPLETE|\n",
      "|       4|2014-07-04 00:00:...|      27181|  PROCESSING|\n",
      "|       5|2014-03-08 00:00:...|      12448|    COMPLETE|\n",
      "+--------+--------------------+-----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: long (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- customer_id: long (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_schema = StructType([\n",
    "    StructField(\"order_id\",LongType()),\n",
    "    StructField(\"order_date\", StringType()),\n",
    "    StructField(\"customer_id\", LongType()),\n",
    "    StructField(\"orders_status\", StringType())\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = new_orders_rdd.toDF(orders_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/27 21:21:08 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 2 (TID 2): Attempting to kill Python Worker\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------+-------------+\n",
      "|order_id|          order_date|customer_id|orders_status|\n",
      "+--------+--------------------+-----------+-------------+\n",
      "|       1|2013-07-27 00:00:...|      30265|       CLOSED|\n",
      "|       2|2013-11-25 00:00:...|      20386|       CLOSED|\n",
      "|       3|2014-01-21 00:00:...|      15768|     COMPLETE|\n",
      "|       4|2014-07-04 00:00:...|      27181|   PROCESSING|\n",
      "|       5|2014-03-08 00:00:...|      12448|     COMPLETE|\n",
      "+--------+--------------------+-----------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: long (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- customer_id: long (nullable = true)\n",
      " |-- orders_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
