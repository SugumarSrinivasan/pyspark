{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inferring the Schema:**\n",
    "\n",
    "Inferring the schema in PySpark allows you to automatically determine the structure (i.e., column names and types) of data, especially when dealing with formats like JSON, CSV, or Parquet. PySpark provides several ways to infer the schema depending on the format of the data.\n",
    "\n",
    "Here’s a guide to inferring the schema in PySpark for common data formats:\n",
    "\n",
    "**1. Inferring Schema from CSV File:**\n",
    "\n",
    "When reading a CSV file in PySpark, you can automatically infer the schema by setting the inferSchema option to True. This means that PySpark will scan the first few rows of the CSV to determine the data types of the columns.\n",
    "```\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"InferSchemaExample\").getOrCreate()\n",
    "\n",
    "# Read CSV file and infer schema\n",
    "df = spark.read.option(\"inferSchema\", \"true\").csv(\"path_to_your_file.csv\", header=True)\n",
    "\n",
    "# Show inferred schema\n",
    "df.printSchema()\n",
    "\n",
    "# Show data\n",
    "df.show()\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "-   header=True: Treats the first row as the header (column names).\n",
    "-   inferSchema=True: Infers the column types based on the data in the CSV file.\n",
    "\n",
    "**2. Inferring Schema from JSON File:**\n",
    "\n",
    "In the case of JSON, PySpark can infer the schema directly from the data, as JSON is a semi-structured format.\n",
    "\n",
    "```\n",
    "# Read JSON file and infer schema\n",
    "df_json = spark.read.json(\"path_to_your_file.json\")\n",
    "\n",
    "# Show inferred schema\n",
    "df_json.printSchema()\n",
    "\n",
    "# Show data\n",
    "df_json.show()\n",
    "```\n",
    "\n",
    "**3. Inferring Schema from Parquet File:**\n",
    "Parquet is a columnar format that already contains schema information, so you don’t need to use inferSchema. PySpark will automatically read the schema when loading Parquet files.\n",
    "\n",
    "```\n",
    "# Read Parquet file\n",
    "df_parquet = spark.read.parquet(\"path_to_your_file.parquet\")\n",
    "\n",
    "# Show inferred schema\n",
    "df_parquet.printSchema()\n",
    "\n",
    "# Show data\n",
    "df_parquet.show()\n",
    "```\n",
    "\n",
    "**Optimizing the inferSchema Process:**\n",
    "\n",
    "Inferring the schema can be costly on large datasets since PySpark needs to scan the data to determine the types. To improve performance:\n",
    "\n",
    "-   Consider using a sample of the data with samplingRatio (e.g., 0.1 for 10% sample).\n",
    "-   Use Parquet or JSON when possible, as they inherently store schema information.\n",
    "\n",
    "```\n",
    "# Example of inferring schema with a sampling ratio\n",
    "df_sampled = spark.read \\\n",
    ".format(\"csv\") \\\n",
    ".option(\"header\",\"true\") \\\n",
    ".option(\"inferSchema\", \"true\") \\\n",
    ".option(\"samplingRatio\", 0.1) \\\n",
    ".load(\"path_to_your_file.csv\")\n",
    "\n",
    "# Show schema and data\n",
    "df_sampled.printSchema()\n",
    "df_sampled.show()\n",
    "```\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "-   **CSV:** Use inferSchema=True to automatically detect data types.\n",
    "-   **JSON:** Schema is inferred directly from the data, no need for inferSchema.\n",
    "-   **Parquet:** No need for schema inference, as schema is stored in the file itself.\n",
    "-   **Custom Schema:** Use StructType if you want to define a schema explicitly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Schema Enforcement:**\n",
    "\n",
    "Schema enforcement in PySpark refers to the process of ensuring that the data in a DataFrame conforms to a predefined structure or schema. This schema defines the column names, data types, and the structure of the data. Schema enforcement is important because it helps ensure data integrity, consistency, and efficient processing when working with large datasets.\n",
    "\n",
    "**Why is Schema Enforcement Important?**\n",
    "\n",
    "-   **1.Data Quality:** It ensures that the data adheres to the expected types and formats, preventing errors during computations or transformations.\n",
    "\n",
    "-   **2.Performance:** PySpark can optimize operations like joins, aggregations, and filters when the schema is well-defined.\n",
    "\n",
    "-   **3.Data Compatibility:** When working with multiple sources, enforcing a schema ensures that the data from different sources aligns properly.\n",
    "\n",
    "-   **4.Validation:** It can help catch any discrepancies early in the data processing pipeline, preventing bad data from being processed further.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init\n",
    "import getpass\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "username = getpass.getuser()\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config(\"spark.sql.catalogImplementation\", \"hive\"). \\\n",
    "    config(\"spark.sql.warehouse.dir\",f\"/Users/{username}/Documents/data/warehouse\"). \\\n",
    "    enableHiveSupport(). \\\n",
    "    master(\"local\"). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    ".format(\"csv\") \\\n",
    ".load(\"/Users/sugumarsrinivasan/Documents/data/orders.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----+----------+\n",
      "|_c0|                 _c1|  _c2|       _c3|\n",
      "+---+--------------------+-----+----------+\n",
      "|  1|2013-07-27 00:00:...|30265|    CLOSED|\n",
      "|  2|2013-11-25 00:00:...|20386|    CLOSED|\n",
      "|  3|2014-01-21 00:00:...|15768|  COMPLETE|\n",
      "|  4|2014-07-04 00:00:...|27181|PROCESSING|\n",
      "|  5|2014-03-08 00:00:...|12448|  COMPLETE|\n",
      "+---+--------------------+-----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining the schema using DDL Method:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_schema = (\"order_id integer, order_date date, customer_id long, order_status string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    ".format(\"csv\") \\\n",
    ".schema(orders_schema) \\\n",
    ".load(\"/Users/sugumarsrinivasan/Documents/data/orders.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------+------------+\n",
      "|order_id|order_date|customer_id|order_status|\n",
      "+--------+----------+-----------+------------+\n",
      "|       1|2013-07-27|      30265|      CLOSED|\n",
      "|       2|2013-11-25|      20386|      CLOSED|\n",
      "|       3|2014-01-21|      15768|    COMPLETE|\n",
      "|       4|2014-07-04|      27181|  PROCESSING|\n",
      "|       5|2014-03-08|      12448|    COMPLETE|\n",
      "+--------+----------+-----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: date (nullable = true)\n",
      " |-- customer_id: long (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if there is a data type issue, then we will get the column values as null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_schema = (\"order_id integer, order_date date, customer_id long, order_status long\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    ".format(\"csv\") \\\n",
    ".schema(orders_schema) \\\n",
    ".load(\"/Users/sugumarsrinivasan/Documents/data/orders.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: date (nullable = true)\n",
      " |-- customer_id: long (nullable = true)\n",
      " |-- order_status: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------+------------+\n",
      "|order_id|order_date|customer_id|order_status|\n",
      "+--------+----------+-----------+------------+\n",
      "|       1|2013-07-27|      30265|        NULL|\n",
      "|       2|2013-11-25|      20386|        NULL|\n",
      "|       3|2014-01-21|      15768|        NULL|\n",
      "|       4|2014-07-04|      27181|        NULL|\n",
      "|       5|2014-03-08|      12448|        NULL|\n",
      "+--------+----------+-----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining the schema using StructType Method:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "orders_schema = StructType([\n",
    "    StructField(\"order_id\", LongType()),\n",
    "    StructField(\"order_date\",DateType()),\n",
    "    StructField(\"customer_id\",LongType()),\n",
    "    StructField(\"order_status\",StringType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    ".format(\"csv\") \\\n",
    ".schema(orders_schema) \\\n",
    ".load(\"/Users/sugumarsrinivasan/Documents/data/orders.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------+------------+\n",
      "|order_id|order_date|customer_id|order_status|\n",
      "+--------+----------+-----------+------------+\n",
      "|       1|2013-07-27|      30265|      CLOSED|\n",
      "|       2|2013-11-25|      20386|      CLOSED|\n",
      "|       3|2014-01-21|      15768|    COMPLETE|\n",
      "|       4|2014-07-04|      27181|  PROCESSING|\n",
      "|       5|2014-03-08|      12448|    COMPLETE|\n",
      "+--------+----------+-----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: long (nullable = true)\n",
      " |-- order_date: date (nullable = true)\n",
      " |-- customer_id: long (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pyspark-env)",
   "language": "python",
   "name": "pyspark-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
