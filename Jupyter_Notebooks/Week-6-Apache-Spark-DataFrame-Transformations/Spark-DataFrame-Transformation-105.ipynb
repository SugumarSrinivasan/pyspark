{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop():**\n",
    "\n",
    "In PySpark, the `drop()` transformation is used to remove one or more columns from a DataFrame. This can be useful when you no longer need certain columns in your DataFrame for analysis or when you're preparing the data for further transformations or writing it to an output.\n",
    "\n",
    "**Syntax:**\n",
    "\n",
    "```\n",
    "DataFrame.drop(*cols)\n",
    "```\n",
    "\n",
    "-   *cols: This is a variable-length argument that specifies the names of the columns to be dropped. You can pass a single column name as a string or multiple column names as a list or multiple string arguments.\n",
    "\n",
    "**Example Usage:**\n",
    "\n",
    "-   **1. Dropping a single column:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init\n",
    "import getpass\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "username = getpass.getuser()\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config(\"spark.sql.catalogImplementation\", \"hive\"). \\\n",
    "    config(\"spark.sql.warehouse.dir\",f\"/Users/{username}/Documents/data/warehouse\"). \\\n",
    "    enableHiveSupport(). \\\n",
    "    master(\"local\"). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample DataFrame\n",
    "data = [(\"Alice\", 25, \"Female\"), (\"Bob\", 30, \"Male\"), (\"Charlie\", 35, \"Male\")]\n",
    "columns = [\"name\", \"age\", \"gender\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+\n",
      "|   name|age|gender|\n",
      "+-------+---+------+\n",
      "|  Alice| 25|Female|\n",
      "|    Bob| 30|  Male|\n",
      "|Charlie| 35|  Male|\n",
      "+-------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show original DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'gender' column\n",
    "df_dropped = df.drop(\"gender\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   name|age|\n",
      "+-------+---+\n",
      "|  Alice| 25|\n",
      "|    Bob| 30|\n",
      "|Charlie| 35|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the DataFrame after dropping the 'gender' column\n",
    "df_dropped.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Dropping multiple columns:**\n",
    "\n",
    "You can also drop multiple columns by passing more than one column name to the drop() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop both 'age' and 'gender' columns\n",
    "df_dropped_multiple = df.drop(\"age\", \"gender\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|  Alice|\n",
      "|    Bob|\n",
      "|Charlie|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the DataFrame after dropping multiple columns\n",
    "df_dropped_multiple.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Notes:**\n",
    "\n",
    "-   The drop() method does not modify the original DataFrame in place. It returns a new DataFrame with the specified columns removed.\n",
    "If you attempt to drop a column that does not exist, PySpark will raise an error.\n",
    "\n",
    "-   This operation is computationally expensive if performed multiple times on large datasets, as it involves shuffling data in the underlying partitions.\n",
    "\n",
    "**Use Case:**\n",
    "\n",
    "The drop() transformation is especially useful when you're working with large datasets and want to reduce the memory footprint or eliminate unnecessary columns before performing operations like filtering, aggregating, or writing the data to storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mastering Column Selection and Expression Evaluation with select() and expr() in PySpark**\n",
    "\n",
    "In PySpark, `select()` is a transformation that allows you to choose specific columns or expressions from a DataFrame. It's similar to a SELECT statement in SQL. You can use select() to:\n",
    "\n",
    "-   Select individual columns from a DataFrame.\n",
    "-   Apply expressions or transformations to one or more columns.\n",
    "-   Rename columns or create new columns.\n",
    "\n",
    "**Syntax:**\n",
    "```\n",
    "df.select(*cols)\n",
    "```\n",
    "\n",
    "-   **df:** The DataFrame you are operating on.\n",
    "-   **cols:** One or more columns or expressions you want to select.\n",
    "\n",
    "You can use the `expr()` function inside select() to apply SQL-like expressions to the columns in your DataFrame. expr() allows you to run SQL-style queries for column transformations, which can include arithmetic operations, string manipulations, and other expressions.\n",
    "\n",
    "The expr() function allows you to perform operations like:\n",
    "\n",
    "-   Mathematical calculations (col1 + col2, col1 * 2, etc.)\n",
    "-   String functions (upper(col1), concat(col1, col2), etc.)\n",
    "-   Conditional operations (CASE WHEN, etc.)\n",
    "\n",
    "**Example:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample DataFrame\n",
    "data = [(\"Alice\", 10), (\"Bob\", 15), (\"Catherine\", 20)]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+\n",
      "|     Name|Age|\n",
      "+---------+---+\n",
      "|    Alice| 10|\n",
      "|      Bob| 15|\n",
      "|Catherine| 20|\n",
      "+---------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|     Name|\n",
      "+---------+\n",
      "|    Alice|\n",
      "|      Bob|\n",
      "|Catherine|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"Name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "|Age|Double_Age|\n",
      "+---+----------+\n",
      "| 10|        20|\n",
      "| 15|        30|\n",
      "| 20|        40|\n",
      "+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"Age\",expr(\"Age * 2 AS Double_Age\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+---------+\n",
      "|     Name|Age|Age_Group|\n",
      "+---------+---+---------+\n",
      "|    Alice| 10|    Child|\n",
      "|      Bob| 15|    Adult|\n",
      "|Catherine| 20|    Adult|\n",
      "+---------+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"Name\",\"Age\", expr(\"CASE WHEN Age > 10 THEN 'Adult' ELSE 'Child' END AS Age_Group\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Points:**\n",
    "\n",
    "-   expr() allows you to write SQL expressions, which gives you flexibility in transforming your data.\n",
    "\n",
    "-   You can use expr() to perform arithmetic, string operations, or conditional logic inside the select() method.\n",
    "\n",
    "-   expr() is particularly useful when you want to apply complex expressions without having to write multiple functions like col(), when(), etc.\n",
    "\n",
    "This makes select() with expr() very powerful for complex column manipulations in PySpark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding the selectExpr() Transformation in PySpark:**\n",
    "\n",
    "In PySpark, the selectExpr() transformation is a powerful way to select and transform columns in a DataFrame using SQL expressions. It allows you to use SQL syntax to specify how you want to select or modify the columns, enabling more complex expressions, calculations, and transformations in a concise and readable manner.\n",
    "\n",
    "**Syntax:**\n",
    "\n",
    "```\n",
    "DataFrame.selectExpr(*expr)\n",
    "```\n",
    "\n",
    "**Parameters:**\n",
    "\n",
    "-   *expr: A list of string expressions or column names. Each string represents a SQL expression, which can include column names, arithmetic operations, aggregations, and SQL functions.\n",
    "\n",
    "**Key Features:**\n",
    "\n",
    "-   **SQL Expressions:** You can use SQL expressions for selecting and transforming the data, similar to how you would write SQL queries.\n",
    "\n",
    "-   **Column Alias:** You can give new names to the columns (aliases) in the result.\n",
    "\n",
    "-   **Complex Operations:** You can apply complex transformations, including mathematical operations, aggregations, and conditional logic.\n",
    "\n",
    "Example 1: Selecting Columns with Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "data = [(1, \"John\", 30), (2, \"Jane\", 25), (3, \"Sam\", 35)]\n",
    "columns = [\"id\", \"name\", \"age\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+---------+\n",
      "| id|Name|Age|Doube_Age|\n",
      "+---+----+---+---------+\n",
      "|  1|John| 30|       60|\n",
      "|  2|Jane| 25|       50|\n",
      "|  3| Sam| 35|       70|\n",
      "+---+----+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\"id\",\"Name\",\"Age\", \"Age * 2 AS Doube_Age\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the selectExpr method is used to select the columns id, name, age and a transformed version of age (age * 2) which is renamed as double_age."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 2: Using SQL Expressions for Complex Logic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+---------+\n",
      "| id|Name|Age|Age_Group|\n",
      "+---+----+---+---------+\n",
      "|  1|John| 30|   Junior|\n",
      "|  2|Jane| 25|   Junior|\n",
      "|  3| Sam| 35|   Senior|\n",
      "+---+----+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\"id\", \n",
    "              \"Name\",\n",
    "              \"Age\",\n",
    "              \"CASE WHEN Age > 30 THEN 'Senior' ELSE 'Junior' END AS Age_Group\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the SQL CASE expression is used to create a new column age_group, which categorizes people as 'Senior' or 'Junior' based on their age."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 3: Performing Aggregations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+\n",
      "|average_age|max_age|\n",
      "+-----------+-------+\n",
      "|       30.0|     35|\n",
      "+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\"avg(Age) AS average_age\", \"max(Age) AS max_age\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we use selectExpr() to calculate the average and maximum age from the DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 4: Renaming Columns**\n",
    "\n",
    "You can also rename columns directly in selectExpr():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+--------+\n",
      "|User_Id|User_Name|User_Age|\n",
      "+-------+---------+--------+\n",
      "|      1|     John|      30|\n",
      "|      2|     Jane|      25|\n",
      "|      3|      Sam|      35|\n",
      "+-------+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\"id as User_Id\", \"Name as User_Name\", \"Age as User_Age\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary:**\n",
    "\n",
    "-   selectExpr() enables the use of SQL-like expressions to select and transform columns.\n",
    "-   You can perform mathematical operations, conditional logic, aggregations, and rename columns within the same transformation.\n",
    "-   It provides a more flexible and concise approach compared to using select() with column objects or expressions directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using `expr()` Inside `withColumn()` for SQL Expressions in PySpark:**\n",
    "\n",
    "you can use the expr() function inside the withColumn() transformation in PySpark.\n",
    "\n",
    "The expr() function allows you to use SQL expressions to define column transformations within PySpark DataFrame operations. This can be useful when you need to perform complex operations or reference columns directly in a SQL-like syntax.\n",
    "\n",
    "**Example of using expr() inside withColumn():**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "data = [(\"John\", 25), (\"Alice\", 30), (\"Bob\", 35)]\n",
    "columns = [\"name\", \"age\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column 'age_double' using expr inside withColumn\n",
    "df_with_age_double = df.withColumn(\"age_double\", expr(\"age * 2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----------+\n",
      "| name|age|age_double|\n",
      "+-----+---+----------+\n",
      "| John| 25|        50|\n",
      "|Alice| 30|        60|\n",
      "|  Bob| 35|        70|\n",
      "+-----+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_with_age_double.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this example:**\n",
    "\n",
    "-   The expr(\"age * 2\") expression multiplies the age column by 2.\n",
    "-   The result is added to a new column called age_double.\n",
    "\n",
    "**Common Use Cases:**\n",
    "\n",
    "-   Mathematical operations (e.g., addition, subtraction, multiplication, division).\n",
    "-   String manipulations.\n",
    "-   Conditional logic (e.g., CASE WHEN statements).\n",
    "-   More complex SQL-style operations that are easier to write with SQL syntax rather than using PySpark's built-in functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use expr() for more advanced SQL expressions like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_column = df.withColumn(\"age_group\", expr(\"CASE WHEN age < 30 THEN 'Young' ELSE 'Old' END\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---------+\n",
      "| name|age|age_group|\n",
      "+-----+---+---------+\n",
      "| John| 25|    Young|\n",
      "|Alice| 30|      Old|\n",
      "|  Bob| 35|      Old|\n",
      "+-----+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_with_column.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This would create a new column called age_group with conditional values based on the age column.\n",
    "\n",
    "**Important Notes:**\n",
    "\n",
    "-   expr() allows you to write SQL-style syntax, but it might not be as efficient as using the equivalent PySpark functions when possible.\n",
    "-   You need to ensure that the SQL expression you write is valid and references the correct column names."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pyspark-env)",
   "language": "python",
   "name": "pyspark-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
