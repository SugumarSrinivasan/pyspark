{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Handling the Non-Standard Date Format in PySpark:**\n",
    "\n",
    "In PySpark, when you're loading data into a DataFrame, the `option(\"dateFormat\", \"\")` is typically used to specify the format of the date columns in the input data. This option helps PySpark correctly parse and interpret the dates in your data according to a specific pattern.\n",
    "\n",
    "Here's how you might use option(\"dateFormat\", \"\") when loading a dataset (like a CSV or JSON) that contains date values:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 1: Reading a CSV file with a custom date format**\n",
    "\n",
    "Let's say your CSV file has a column of dates in the format MM-dd-yyyy, and you want to load the data into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,07-27-2013,30265,CLOSED\n",
      "2,11-25-2013,20386,CLOSED\n",
      "3,01-15-2014,15768,COMPLETE\n",
      "4,07-14-2014,27181,PROCESSING\n",
      "5,03-08-2014,12448,COMPLETE\n",
      "6,08-20-2014,49340,CLOSED\n",
      "7,09-12-2014,13801,PROCESSING\n",
      "8,04-23-2014,28523,PENDING_PAYMENT\n",
      "9,07-01-2014,26329,CLOSED\n",
      "10,07-29-2013,38797,COMPLETE\n"
     ]
    }
   ],
   "source": [
    "! cat /Users/sugumarsrinivasan/Documents/data/orders_sample2.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init\n",
    "import getpass\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "username = getpass.getuser()\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config(\"spark.sql.catalogImplementation\", \"hive\"). \\\n",
    "    config(\"spark.sql.warehouse.dir\",f\"/Users/{username}/Documents/data/warehouse\"). \\\n",
    "    enableHiveSupport(). \\\n",
    "    master(\"local\"). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_schema = (\"order_id integer, order_date date, customer_id long, order_status string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    ".format(\"csv\") \\\n",
    ".option(\"dateFormat\",\"MM-dd-yyyy\") \\\n",
    ".schema(orders_schema) \\\n",
    ".load(\"/Users/sugumarsrinivasan/Documents/data/orders_sample2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "-   option(\"dateFormat\", \"MM-dd-yyyy\"): This option tells PySpark to interpret the date columns in the format MM-dd-yyyy.\n",
    "-   schema(orders_schema): This option tells PySpark to enforce the schema, defined in orders_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------+---------------+\n",
      "|order_id|order_date|customer_id|   order_status|\n",
      "+--------+----------+-----------+---------------+\n",
      "|       1|2013-07-27|      30265|         CLOSED|\n",
      "|       2|2013-11-25|      20386|         CLOSED|\n",
      "|       3|2014-01-15|      15768|       COMPLETE|\n",
      "|       4|2014-07-14|      27181|     PROCESSING|\n",
      "|       5|2014-03-08|      12448|       COMPLETE|\n",
      "|       6|2014-08-20|      49340|         CLOSED|\n",
      "|       7|2014-09-12|      13801|     PROCESSING|\n",
      "|       8|2014-04-23|      28523|PENDING_PAYMENT|\n",
      "|       9|2014-07-01|      26329|         CLOSED|\n",
      "|      10|2013-07-29|      38797|       COMPLETE|\n",
      "+--------+----------+-----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: date (nullable = true)\n",
      " |-- customer_id: long (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:**\n",
    "\n",
    "-   The date format string should be compatible with Java's SimpleDateFormat. Common date patterns include:\n",
    "\n",
    "    -   yyyy-MM-dd for a date like 2024-12-27\n",
    "    -   MM/dd/yyyy for a date like 12/27/2024\n",
    "    -   yyyy-MM-dd'T'HH:mm:ss for a timestamp like 2024-12-27T14:30:00\n",
    "\n",
    "-   You can also specify additional options like timestampFormat if you are dealing with timestamp columns instead of just dates.\n",
    "\n",
    "**Use cases:**\n",
    "\n",
    "-   This is useful when the date format in your data does not match the default yyyy-MM-dd format used by PySpark, and you want to make sure that the dates are parsed correctly.\n",
    "\n",
    "-   It also helps if you're dealing with non-standard date formats in large datasets and need to optimize the reading process without needing to perform extra transformations after loading the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are working with date columns in a non-standard format and want an alternative way to handle them in PySpark (besides using the option(\"dateFormat\", \"\") when loading data), there are a few other approaches you can take. These methods are useful when you don't want to rely on the loading options, or if you need to do further transformations or custom parsing.\n",
    "\n",
    "**Using `to_date()` Functions:**\n",
    "\n",
    "PySpark provides the `to_date()` and `to_timestamp()` functions to convert a string column into a date or timestamp type. You can specify the custom format directly within these functions.\n",
    "\n",
    "**Example: Using to_date for Date Conversion**\n",
    "\n",
    "Let's say you have a string column date_column with the format MM-dd-yyyy, and you want to convert it into a proper DateType column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_schema = (\"order_id integer, order_date string, customer_id long, order_status string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    ".format(\"csv\") \\\n",
    ".schema(orders_schema) \\\n",
    ".load(\"/Users/sugumarsrinivasan/Documents/data/orders_sample2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- customer_id: long (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------+---------------+\n",
      "|order_id|order_date|customer_id|   order_status|\n",
      "+--------+----------+-----------+---------------+\n",
      "|       1|07-27-2013|      30265|         CLOSED|\n",
      "|       2|11-25-2013|      20386|         CLOSED|\n",
      "|       3|01-15-2014|      15768|       COMPLETE|\n",
      "|       4|07-14-2014|      27181|     PROCESSING|\n",
      "|       5|03-08-2014|      12448|       COMPLETE|\n",
      "|       6|08-20-2014|      49340|         CLOSED|\n",
      "|       7|09-12-2014|      13801|     PROCESSING|\n",
      "|       8|04-23-2014|      28523|PENDING_PAYMENT|\n",
      "|       9|07-01-2014|      26329|         CLOSED|\n",
      "|      10|07-29-2013|      38797|       COMPLETE|\n",
      "+--------+----------+-----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It creates an additional column with the name 'order_date_new' and update the data type in the newly added column.\n",
    "\n",
    "new_df = df.withColumn(\"order_date_new\", to_date(\"order_date\", \"MM-dd-yyyy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------+---------------+--------------+\n",
      "|order_id|order_date|customer_id|   order_status|order_date_new|\n",
      "+--------+----------+-----------+---------------+--------------+\n",
      "|       1|07-27-2013|      30265|         CLOSED|    2013-07-27|\n",
      "|       2|11-25-2013|      20386|         CLOSED|    2013-11-25|\n",
      "|       3|01-15-2014|      15768|       COMPLETE|    2014-01-15|\n",
      "|       4|07-14-2014|      27181|     PROCESSING|    2014-07-14|\n",
      "|       5|03-08-2014|      12448|       COMPLETE|    2014-03-08|\n",
      "|       6|08-20-2014|      49340|         CLOSED|    2014-08-20|\n",
      "|       7|09-12-2014|      13801|     PROCESSING|    2014-09-12|\n",
      "|       8|04-23-2014|      28523|PENDING_PAYMENT|    2014-04-23|\n",
      "|       9|07-01-2014|      26329|         CLOSED|    2014-07-01|\n",
      "|      10|07-29-2013|      38797|       COMPLETE|    2013-07-29|\n",
      "+--------+----------+-----------+---------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- customer_id: long (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_date_new: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It make the changes in the exiting column without creating any additional columns in the dataframe\n",
    "\n",
    "new_df = df.withColumn(\"order_date\", to_date(\"order_date\", \"MM-dd-yyyy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------+---------------+\n",
      "|order_id|order_date|customer_id|   order_status|\n",
      "+--------+----------+-----------+---------------+\n",
      "|       1|2013-07-27|      30265|         CLOSED|\n",
      "|       2|2013-11-25|      20386|         CLOSED|\n",
      "|       3|2014-01-15|      15768|       COMPLETE|\n",
      "|       4|2014-07-14|      27181|     PROCESSING|\n",
      "|       5|2014-03-08|      12448|       COMPLETE|\n",
      "|       6|2014-08-20|      49340|         CLOSED|\n",
      "|       7|2014-09-12|      13801|     PROCESSING|\n",
      "|       8|2014-04-23|      28523|PENDING_PAYMENT|\n",
      "|       9|2014-07-01|      26329|         CLOSED|\n",
      "|      10|2013-07-29|      38797|       COMPLETE|\n",
      "+--------+----------+-----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: date (nullable = true)\n",
      " |-- customer_id: long (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary:**\n",
    "\n",
    "-   to_date() and to_timestamp() are the primary functions for handling non-standard date and timestamp formats by specifying the format directly.\n",
    "\n",
    "By using one or more of these methods, you can flexibly handle dates and timestamps in any format without relying solely on the option(\"dateFormat\", \"\") approach when loading the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Handling Inconsistent Data Types in Spark DataFrames: Dealing with Type Mismatch in Columns**\n",
    "\n",
    "When you try to load a dataset into a Spark DataFrame with a column that has inconsistent types (e.g., a column defined as LongType but containing some rows with string values), Spark will encounter a problem while reading that data.\n",
    "\n",
    "Here’s what typically happens:\n",
    "\n",
    "**1. Type Mismatch (Error or Null values):**\n",
    "\n",
    "-   If you specify the schema explicitly (e.g., using LongType for customer_id), Spark will attempt to cast the data to LongType.\n",
    "\n",
    "    -   For rows where the customer_id is a string that cannot be converted to a long integer, Spark will throw a java.lang.NumberFormatException or a similar error during the loading process.\n",
    "    -   Alternatively, Spark might replace those rows with null values if the string cannot be cast to LongType (depending on the error handling configuration).\n",
    "\n",
    "**2. Schema Inference (DataFrame without explicit schema):**\n",
    "\n",
    "-   If you rely on schema inference (without specifying the schema manually), Spark will try to infer the schema based on the data it reads.\n",
    "\n",
    "    -   If most of the rows can be cast to LongType but a few contain strings, Spark might infer the column type as StringType to accommodate all values, which is less restrictive but may result in loss of intended data consistency.\n",
    "    -   In this case, your customer_id column will be treated as StringType instead of LongType.\n",
    "\n",
    "**3. Handling Data Conversion Errors:**\n",
    "\n",
    "-   You can use some strategies to handle type conversion errors or unexpected data types:\n",
    "\n",
    "    -   Using option(\"mode\", \"DROPMALFORMED\"): This will drop any rows that cannot be parsed correctly based on the schema.\n",
    "    -   Using option(\"mode\", \"PERMISSIVE\") (default behavior): Spark will try to parse as many rows as possible, and the ones that fail will be set to null.\n",
    "    -   Using option(\"mode\", \"FAILFAST\"): This will cause the job to fail immediately upon encountering any corrupt record.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "If you're loading data from a CSV file and you specify LongType for the customer_id column, you could see the following behavior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,2013-07-27,30265,CLOSED\n",
      "2,2013-11-25,20386,CLOSED\n",
      "3,2014-01-21,15768,COMPLETE\n",
      "4,2014-07-04,27181,PROCESSING\n",
      "5,2014-03-08,unknown,COMPLETE\n",
      "6,2014-07-20,49340,CLOSED\n",
      "7,2013-12-14,13801,PROCESSING\n",
      "8,2014-04-23,error,PENDING_PAYMENT\n",
      "9,2014-01-07,26329,CLOSED\n",
      "10,2013-07-29,38797,COMPLETE\n"
     ]
    }
   ],
   "source": [
    "! cat /Users/sugumarsrinivasan/Documents/data/orders_sample3.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_schema = 'order_id integer, order_date string, customer_id long, order_status string'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    ".format(\"csv\") \\\n",
    ".schema(orders_schema) \\\n",
    ".load(\"/Users/sugumarsrinivasan/Documents/data/orders_sample3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------+---------------+\n",
      "|order_id|order_date|customer_id|   order_status|\n",
      "+--------+----------+-----------+---------------+\n",
      "|       1|2013-07-27|      30265|         CLOSED|\n",
      "|       2|2013-11-25|      20386|         CLOSED|\n",
      "|       3|2014-01-21|      15768|       COMPLETE|\n",
      "|       4|2014-07-04|      27181|     PROCESSING|\n",
      "|       5|2014-03-08|       NULL|       COMPLETE|\n",
      "|       6|2014-07-20|      49340|         CLOSED|\n",
      "|       7|2013-12-14|      13801|     PROCESSING|\n",
      "|       8|2014-04-23|       NULL|PENDING_PAYMENT|\n",
      "|       9|2014-01-07|      26329|         CLOSED|\n",
      "|      10|2013-07-29|      38797|       COMPLETE|\n",
      "+--------+----------+-----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    ".format(\"csv\") \\\n",
    ".schema(orders_schema) \\\n",
    ".option(\"mode\",\"dropmalformed\") \\\n",
    ".load(\"/Users/sugumarsrinivasan/Documents/data/orders_sample3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------+------------+\n",
      "|order_id|order_date|customer_id|order_status|\n",
      "+--------+----------+-----------+------------+\n",
      "|       1|2013-07-27|      30265|      CLOSED|\n",
      "|       2|2013-11-25|      20386|      CLOSED|\n",
      "|       3|2014-01-21|      15768|    COMPLETE|\n",
      "|       4|2014-07-04|      27181|  PROCESSING|\n",
      "|       6|2014-07-20|      49340|      CLOSED|\n",
      "|       7|2013-12-14|      13801|  PROCESSING|\n",
      "|       9|2014-01-07|      26329|      CLOSED|\n",
      "|      10|2013-07-29|      38797|    COMPLETE|\n",
      "+--------+----------+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    ".format(\"csv\") \\\n",
    ".schema(orders_schema) \\\n",
    ".option(\"mode\",\"failfast\") \\\n",
    ".load(\"/Users/sugumarsrinivasan/Documents/data/orders_sample3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()\n",
    "\n",
    "#output: org.apache.spark.SparkException: [MALFORMED_RECORD_IN_PARSING.WITHOUT_SUGGESTION] Malformed records are detected in record parsing: [5,2014-03-08,null,COMPLETE].\n",
    "#Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are non-numeric values in the customer_id column, those rows will either be skipped (if DROPMALFORMED is used) or set to null if PERMISSIVE mode is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary:**\n",
    "\n",
    "-   If the data is inconsistent (some rows are strings while others are valid long numbers), Spark may either throw an error or convert the column type to StringType during schema inference.\n",
    "\n",
    "-   It's important to handle these issues by specifying an appropriate schema or setting the correct error-handling mode when reading the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
