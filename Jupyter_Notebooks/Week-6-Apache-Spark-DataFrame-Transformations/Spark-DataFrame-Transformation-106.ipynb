{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Handling Duplicate Rows in PySpark DataFrames: Methods and Examples**\n",
    "\n",
    "Handling duplicate rows in a DataFrame is a common operation in data processing. In PySpark, this can be done using several methods provided by the pyspark.sql.DataFrame API. The most commonly used methods for handling duplicates are `dropDuplicates()` and `distinct()`. Let's look at examples of how to use these methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Removing Duplicate Rows Using dropDuplicates()**\n",
    "\n",
    "The dropDuplicates() method removes duplicate rows based on all columns or a subset of columns.\n",
    "\n",
    "**Example 1: Removing Duplicates Based on All Columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init\n",
    "import getpass\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "username = getpass.getuser()\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config(\"spark.sql.catalogImplementation\", \"hive\"). \\\n",
    "    config(\"spark.sql.warehouse.dir\",f\"/Users/{username}/Documents/data/warehouse\"). \\\n",
    "    enableHiveSupport(). \\\n",
    "    master(\"local\"). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "data = [\n",
    "    (\"Alice\", 30, \"F\"),\n",
    "    (\"Bob\", 25, \"M\"),\n",
    "    (\"Alice\", 30, \"F\"),  # Duplicate row\n",
    "    (\"Charlie\", 35, \"M\")\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\", \"Gender\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+\n",
      "|   Name|Age|Gender|\n",
      "+-------+---+------+\n",
      "|  Alice| 30|     F|\n",
      "|    Bob| 25|     M|\n",
      "|  Alice| 30|     F|\n",
      "|Charlie| 35|     M|\n",
      "+-------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show original DataFrame\n",
    "print(\"Original DataFrame:\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate rows based on all columns\n",
    "df_no_duplicates = df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame after removing duplicates:\n",
      "+-------+---+------+\n",
      "|   Name|Age|Gender|\n",
      "+-------+---+------+\n",
      "|  Alice| 30|     F|\n",
      "|    Bob| 25|     M|\n",
      "|Charlie| 35|     M|\n",
      "+-------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show DataFrame after removing duplicates\n",
    "print(\"DataFrame after removing duplicates:\")\n",
    "df_no_duplicates.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the row (\"Alice\", 30, \"F\") was duplicated, and the dropDuplicates() method removed one of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 2: Removing Duplicates Based on Specific Columns**\n",
    "\n",
    "You can also remove duplicates based on a subset of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+\n",
      "|   Name|Age|Gender|\n",
      "+-------+---+------+\n",
      "|  Alice| 30|     F|\n",
      "|    Bob| 25|     M|\n",
      "|Charlie| 35|     M|\n",
      "+-------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicates based on specific columns (e.g., \"Name\")\n",
    "df_no_duplicates_name = df.dropDuplicates([\"Name\"])\n",
    "\n",
    "# Show DataFrame after removing duplicates based on \"Name\" column\n",
    "df_no_duplicates_name.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, only the first occurrence of each Name is kept, and duplicates based on the Name column are removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Using distinct() to Remove Duplicates**\n",
    "\n",
    "The distinct() method removes duplicate rows based on all columns, similar to dropDuplicates() but without the option to specify a subset of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+\n",
      "|   Name|Age|Gender|\n",
      "+-------+---+------+\n",
      "|  Alice| 30|     F|\n",
      "|    Bob| 25|     M|\n",
      "|Charlie| 35|     M|\n",
      "+-------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_distinct_rows = df.distinct()\n",
    "df_distinct_rows.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the result is the same as using dropDuplicates(), as distinct() considers all columns when removing duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Handling Duplicates with Conditions**\n",
    "\n",
    "Sometimes, you might want to keep the first occurrence of a duplicate row, or perhaps aggregate the data before removing duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to keep the first row of each duplicate group based on a certain condition (e.g., keep the most recent row based on a timestamp), you can use the groupBy() method followed by an aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data with a timestamp column\n",
    "data_with_timestamp = [\n",
    "    (\"Alice\", 30, \"F\", \"2023-01-01\"),\n",
    "    (\"Bob\", 25, \"M\", \"2023-01-02\"),\n",
    "    (\"Alice\", 30, \"F\", \"2023-01-02\"),  # Duplicate row with a different timestamp\n",
    "    (\"Charlie\", 35, \"M\", \"2023-01-01\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame with timestamp column\n",
    "df_timestamp = spark.createDataFrame(data_with_timestamp, [\"Name\", \"Age\", \"Gender\", \"Timestamp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "+-------+---+------+----------+\n",
      "|   Name|Age|Gender| Timestamp|\n",
      "+-------+---+------+----------+\n",
      "|  Alice| 30|     F|2023-01-01|\n",
      "|    Bob| 25|     M|2023-01-02|\n",
      "|  Alice| 30|     F|2023-01-02|\n",
      "|Charlie| 35|     M|2023-01-01|\n",
      "+-------+---+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show original DataFrame\n",
    "print(\"Original DataFrame:\")\n",
    "df_timestamp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use groupBy to remove duplicates and keep the most recent Timestamp for each Name\n",
    "df_grouped = df_timestamp.groupBy(\"Name\", \"Age\", \"Gender\").agg(\n",
    "    F.max(\"Timestamp\").alias(\"LatestTimestamp\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame after removing duplicates with aggregation:\n",
      "+-------+---+------+---------------+\n",
      "|   Name|Age|Gender|LatestTimestamp|\n",
      "+-------+---+------+---------------+\n",
      "|  Alice| 30|     F|     2023-01-02|\n",
      "|    Bob| 25|     M|     2023-01-02|\n",
      "|Charlie| 35|     M|     2023-01-01|\n",
      "+-------+---+------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show DataFrame after grouping and aggregation\n",
    "print(\"DataFrame after removing duplicates with aggregation:\")\n",
    "df_grouped.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we used groupBy() and max() to keep the most recent timestamp for each Name, effectively removing duplicates but retaining the most recent entry for each person."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a complete example with sample data to demonstrate the use of the row_number() function to handle duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "data = [\n",
    "    (\"Alice\", 30, \"F\", \"2023-01-01\"),\n",
    "    (\"Bob\", 25, \"M\", \"2023-01-02\"),\n",
    "    (\"Alice\", 30, \"F\", \"2023-01-02\"),  # Duplicate row\n",
    "    (\"Charlie\", 35, \"M\", \"2023-01-01\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\", \"Gender\", \"Timestamp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+----------+\n",
      "|   Name|Age|Gender| Timestamp|\n",
      "+-------+---+------+----------+\n",
      "|  Alice| 30|     F|2023-01-01|\n",
      "|    Bob| 25|     M|2023-01-02|\n",
      "|Charlie| 35|     M|2023-01-01|\n",
      "+-------+---+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "# Define a window specification to partition by \"Name\" and \"Age\", and order by \"Timestamp\"\n",
    "window_spec = Window.partitionBy(\"Name\", \"Age\").orderBy(\"Timestamp\")\n",
    "\n",
    "# Add a row number column based on the window specification\n",
    "df_with_row_num = df.withColumn(\"row_num\", row_number().over(window_spec))\n",
    "\n",
    "# Filter to keep only the first occurrence of each duplicate group\n",
    "df_cleaned = df_with_row_num.filter(df_with_row_num.row_num == 1).drop(\"row_num\")\n",
    "\n",
    "# Show the cleaned DataFrame\n",
    "df_cleaned.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+----------+\n",
      "|   Name|Age|Gender| Timestamp|\n",
      "+-------+---+------+----------+\n",
      "|  Alice| 30|     F|2023-01-02|\n",
      "|    Bob| 25|     M|2023-01-02|\n",
      "|Charlie| 35|     M|2023-01-01|\n",
      "+-------+---+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define window specification to partition by \"Name\" and \"Age\", and order by \"Timestamp\" in descending order\n",
    "window_spec_desc = Window.partitionBy(\"Name\", \"Age\").orderBy(df[\"Timestamp\"].desc())\n",
    "\n",
    "# Assign row numbers based on the descending order of Timestamp (latest first)\n",
    "df_with_row_num_desc = df.withColumn(\"row_num\", row_number().over(window_spec_desc))\n",
    "\n",
    "# Filter to keep only the latest occurrence (row_num == 1)\n",
    "df_cleaned_latest = df_with_row_num_desc.filter(df_with_row_num_desc.row_num == 1).drop(\"row_num\")\n",
    "\n",
    "# Show the cleaned DataFrame with the latest Timestamp for each person\n",
    "df_cleaned_latest.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach is particularly useful when you want to keep the first occurrence based on a certain column (e.g., Timestamp or any other column you use for ordering) and remove other duplicates within the same group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:**\n",
    "\n",
    "-   dropDuplicates(): Removes duplicates based on all columns or a specified subset of columns.\n",
    "-   distinct(): Removes duplicates based on all columns (equivalent to dropDuplicates() without column specification).\n",
    "-   groupBy() + Aggregation: Allows you to remove duplicates and apply custom logic, such as keeping the most recent entry.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pyspark-env)",
   "language": "python",
   "name": "pyspark-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
