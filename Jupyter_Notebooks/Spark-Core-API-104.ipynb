{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reduceByKey() Vs groupByKey() in pyspark:\n",
    "\n",
    "| **Aspect**               | **`reduceByKey()`**                                         | **`groupByKey()`**                                         |\n",
    "|--------------------------|------------------------------------------------------------|-----------------------------------------------------------|\n",
    "| **Purpose**              | Combines values with the same key using a specified reduce function. | Groups values with the same key into a list or iterable.   |\n",
    "| **Operation Type**       | Transformation that reduces the values of each key.        | Transformation that groups values of each key.            |\n",
    "| **Efficiency**           | More efficient for large datasets as it reduces data during shuffle. | Less efficient; stores all values in memory and then groups them. |\n",
    "| **Shuffle Behavior**     | Causes a shuffle but performs aggregation (combining values) during shuffle. | Causes a shuffle without any aggregation, just grouping.   |\n",
    "| **Output Format**        | Returns an RDD of key-value pairs, where each key is associated with a reduced value. | Returns an RDD of key-value pairs, where each key is associated with an iterable of values. |\n",
    "| **Use Case**             | Use when you want to perform an aggregation (e.g., sum, max, etc.) on values for each key. | Use when you need to collect all values for each key, without aggregation. |\n",
    "| **Memory Usage**         | More memory efficient because values are reduced during the shuffle. | Can consume more memory because it retains all values for each key before processing. |\n",
    "| **Typical Function**     | `reduceByKey(func)` where `func` is a commutative and associative function (e.g., `lambda x, y: x + y`). | `groupByKey()` which groups values associated with each key into a list. |\n",
    "| **Example**              | `rdd.reduceByKey(lambda x, y: x + y)`                      | `rdd.groupByKey()`                                          |\n",
    "\n",
    "**Key Takeaways:**\n",
    "- **`reduceByKey()`** is typically preferred when performing any kind of aggregation (like sum, max, etc.), as it reduces data during the shuffle and is more memory efficient.\n",
    "- **`groupByKey()`** is used when you want to group all the values for each key without any reduction. It can be less efficient because it involves shuffling the entire dataset and storing the entire list of values for each key in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reduceByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySparkExample\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_rdd = spark.sparkContext.textFile(\"/Users/sugumarsrinivasan/Documents/data/orders_4gb.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = base_rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_without_header = base_rdd.filter(lambda line: line != header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_without_header.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_rdd = data_without_header.map(lambda x: (x.split(\",\")[3],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_rdd = mapped_rdd.reduceByKey(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "groupByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySparkExample\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_rdd = spark.sparkContext.textFile(\"/Users/sugumarsrinivasan/Documents/data/orders.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = base_rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_without_header = base_rdd.filter(lambda line: line != header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_rdd = data_without_header.map(lambda x: (x.split(\",\")[3], x.split([2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_rdd = mapped_rdd.groupByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = grouped_rdd.map(lambda x: (x[0],len(x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
