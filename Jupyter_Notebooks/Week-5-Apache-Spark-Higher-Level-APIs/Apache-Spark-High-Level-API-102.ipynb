{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**More About Dataframe reader API:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/25 13:36:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init\n",
    "import getpass\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "username = getpass.getuser()\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config(\"spark.sql.catalogImplementation\", \"hive\"). \\\n",
    "    config(\"spark.sql.warehouse.dir\",f\"/Users/{username}/Documents/data/warehouse\"). \\\n",
    "    enableHiveSupport(). \\\n",
    "    master(\"local\"). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df = spark.read \\\n",
    ".format(\"csv\") \\\n",
    ".option(\"header\", \"true\") \\\n",
    ".option(\"inferSchema\", \"true\") \\\n",
    ".load(\"/Users/sugumarsrinivasan/Documents/data/orders.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------+------------+\n",
      "|order_id|         order_date|customer_id|order_status|\n",
      "+--------+-------------------+-----------+------------+\n",
      "|       1|2013-07-27 00:00:00|      30265|      CLOSED|\n",
      "|       2|2013-11-25 00:00:00|      20386|      CLOSED|\n",
      "|       3|2014-01-21 00:00:00|      15768|    COMPLETE|\n",
      "|       4|2014-07-04 00:00:00|      27181|  PROCESSING|\n",
      "|       5|2014-03-08 00:00:00|      12448|    COMPLETE|\n",
      "+--------+-------------------+-----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PySpark, there are several shortcut methods that allow you to read data from different formats like CSV, JSON, ORC, Parquet, JDBC, and Tables. These methods make it easy to load data into Spark DataFrames for further processing.\n",
    "\n",
    "Let's go through each of these formats and the methods to read them:\n",
    "\n",
    "**1. CSV (Comma Separated Values)**\n",
    "\n",
    "The csv() method is used to read CSV files. You can provide options such as specifying delimiters, header row, schema, and more.\n",
    "\n",
    "**Syntax:**\n",
    "\n",
    "```\n",
    "df = spark.read.csv(\"path_to_file.csv\", header=True, inferSchema=True)\n",
    "```\n",
    "\n",
    "**Key Options:**\n",
    "-   **header=True:** Tells Spark to use the first row as column names.\n",
    "-   **inferSchema=True:** Automatically infers the data types of columns.\n",
    "-   **sep=<delimiter>:** Specifies a custom delimiter (default is comma).\n",
    "-   **quote=\"<quote character>\":** Defines how quoted strings are handled.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```\n",
    "df = spark.read.csv(\"/path/to/file.csv\", header=True, inferSchema=True)\n",
    "df.show()\n",
    "```\n",
    "\n",
    "**2. JSON (JavaScript Object Notation)**\n",
    "\n",
    "The json() method is used to read JSON files, which are commonly used for data exchange.\n",
    "\n",
    "**Syntax:**\n",
    "\n",
    "```\n",
    "df = spark.read.json(\"path_to_file.json\")\n",
    "```\n",
    "\n",
    "**Key Options:**\n",
    "\n",
    "-   **multiline=True:** Allows reading JSON files that span multiple lines (useful for pretty-printed JSON).\n",
    "-   **primitivesAsString=True:** Treats primitive types like strings when reading JSON objects.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```\n",
    "df = spark.read.json(\"/path/to/file.json\")\n",
    "df.show()\n",
    "```\n",
    "\n",
    "**3. ORC (Optimized Row Columnar)**\n",
    "\n",
    "ORC is a columnar storage format that provides high compression and high performance.\n",
    "\n",
    "**Syntax:**\n",
    "\n",
    "```\n",
    "df = spark.read.orc(\"path_to_file.orc\")\n",
    "```\n",
    "\n",
    "**Key Options:**\n",
    "-   ORC files are efficient for both reading and writing, especially in a Hive or SparkSQL environment.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```\n",
    "df = spark.read.orc(\"/path/to/file.orc\")\n",
    "df.show()\n",
    "```\n",
    "\n",
    "**4. Parquet**\n",
    "\n",
    "Parquet is a popular columnar storage format that is optimized for large-scale data processing with Spark. It's very efficient for storage and query performance.\n",
    "\n",
    "**Syntax:**\n",
    "\n",
    "```\n",
    "df = spark.read.parquet(\"path_to_file.parquet\")\n",
    "```\n",
    "\n",
    "**Key Options:**\n",
    "-   **mergeSchema=True:** Merges the schema of multiple Parquet files.\n",
    "-   **dateMetadata=True:** Retains metadata for date and timestamp columns.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```\n",
    "df = spark.read.parquet(\"/path/to/file.parquet\")\n",
    "df.show()\n",
    "```\n",
    "\n",
    "**5. Tables (Spark SQL / Hive tables)**\n",
    "\n",
    "You can read data directly from Spark SQL tables (either from the default Spark catalog or Hive).\n",
    "\n",
    "**Syntax:**\n",
    "\n",
    "```\n",
    "df = spark.read.table(\"table_name\")\n",
    "```\n",
    "\n",
    "-   This reads a table as a DataFrame. If you're working with Hive, Spark will connect to the Hive Metastore to retrieve the table schema and data.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```\n",
    "df = spark.read.table(\"my_table\")\n",
    "df.show()\n",
    "```\n",
    "\n",
    "**6. JDBC (Java Database Connectivity)**\n",
    "\n",
    "To read data from relational databases such as MySQL, PostgreSQL, SQL Server, etc., you can use the jdbc() method.\n",
    "\n",
    "**Syntax:**\n",
    "\n",
    "```\n",
    "df = spark.read.jdbc(url=\"jdbc:<database_url>\", table=\"<table_name>\", properties={\"user\": \"<username>\", \"password\": \"<password>\"})\n",
    "```\n",
    "\n",
    "**Key Options:**\n",
    "\n",
    "-   **url:** JDBC URL to connect to the database.\n",
    "-   **table:** The name of the table you want to read from.\n",
    "-   **properties:** Properties like username and password for connecting to the database.\n",
    "\n",
    "**Example (Reading from MySQL):**\n",
    "\n",
    "```\n",
    "jdbc_url = \"jdbc:mysql://localhost:3306/mydatabase\"\n",
    "properties = {\"user\": \"root\", \"password\": \"password\", \"driver\": \"com.mysql.cj.jdbc.Driver\"}\n",
    "\n",
    "df = spark.read.jdbc(url=jdbc_url, table=\"my_table\", properties=properties)\n",
    "df.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary Table:**\n",
    "\n",
    "| **Format**   | **Method**               | **Use Case**                                         | **Key Options**                        |\n",
    "|--------------|--------------------------|-----------------------------------------------------|----------------------------------------|\n",
    "| CSV          | `spark.read.csv()`        | For reading CSV files                              | `header`, `inferSchema`, `sep`, `quote`|\n",
    "| JSON         | `spark.read.json()`       | For reading JSON files                             | `multiline`, `primitivesAsString`      |\n",
    "| ORC          | `spark.read.orc()`        | Efficient columnar format, often used with Hive    | -                                      |\n",
    "| Parquet      | `spark.read.parquet()`    | Optimized columnar storage format, fast for queries| `mergeSchema`, `dateMetadata`          |\n",
    "| Tables       | `spark.read.table()`      | Read from Spark SQL or Hive tables                 | -                                      |\n",
    "| JDBC         | `spark.read.jdbc()`       | For connecting to relational databases (e.g., MySQL, PostgreSQL) | `url`, `properties`, `table`           |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df = spark.read.csv(\"/Users/sugumarsrinivasan/Documents/data/orders.csv\", header=\"true\", inferSchema=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------------------+---+---------------+\n",
      "|40322|2014-04-08T00:00:00.000+05:30|  1|PENDING_PAYMENT|\n",
      "+-----+-----------------------------+---+---------------+\n",
      "|35390|          2014-05-20 00:00:00|  2|       COMPLETE|\n",
      "|29108|          2013-10-14 00:00:00|  3|PENDING_PAYMENT|\n",
      "|34419|          2014-01-04 00:00:00|  4|         CLOSED|\n",
      "| 9936|          2014-02-16 00:00:00|  5|PENDING_PAYMENT|\n",
      "|41598|          2014-05-02 00:00:00|  6|       COMPLETE|\n",
      "+-----+-----------------------------+---+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df = spark.read.json(\"/Users/sugumarsrinivasan/Documents/data/orders.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------+---------------+\n",
      "|customer_id|          order_date|order_id|   order_status|\n",
      "+-----------+--------------------+--------+---------------+\n",
      "|      40322|2014-04-08T00:00:...|       1|PENDING_PAYMENT|\n",
      "|      35390|2014-05-20T00:00:...|       2|       COMPLETE|\n",
      "|      29108|2013-10-14T00:00:...|       3|PENDING_PAYMENT|\n",
      "|      34419|2014-01-04T00:00:...|       4|         CLOSED|\n",
      "|       9936|2014-02-16T00:00:...|       5|PENDING_PAYMENT|\n",
      "+-----------+--------------------+--------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df = spark.read.orc(\"/Users/sugumarsrinivasan/Documents/data/orders.orc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------+---------------+\n",
      "|customer_id|          order_date|order_id|   order_status|\n",
      "+-----------+--------------------+--------+---------------+\n",
      "|      40322|2014-04-08T00:00:...|       1|PENDING_PAYMENT|\n",
      "|      35390|2014-05-20T00:00:...|       2|       COMPLETE|\n",
      "|      29108|2013-10-14T00:00:...|       3|PENDING_PAYMENT|\n",
      "|      34419|2014-01-04T00:00:...|       4|         CLOSED|\n",
      "|       9936|2014-02-16T00:00:...|       5|PENDING_PAYMENT|\n",
      "+-----------+--------------------+--------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df = spark.read.parquet(\"/Users/sugumarsrinivasan/Documents/data/orders.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------+---------------+\n",
      "|customer_id|          order_date|order_id|   order_status|\n",
      "+-----------+--------------------+--------+---------------+\n",
      "|      40322|2014-04-08T00:00:...|       1|PENDING_PAYMENT|\n",
      "|      35390|2014-05-20T00:00:...|       2|       COMPLETE|\n",
      "|      29108|2013-10-14T00:00:...|       3|PENDING_PAYMENT|\n",
      "|      34419|2014-01-04T00:00:...|       4|         CLOSED|\n",
      "|       9936|2014-02-16T00:00:...|       5|PENDING_PAYMENT|\n",
      "+-----------+--------------------+--------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = orders_df.where(\"customer_id = 35390\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------+---------------+\n",
      "|order_id|order_date         |customer_id|order_status   |\n",
      "+--------+-------------------+-----------+---------------+\n",
      "|2       |2014-05-20 00:00:00|35390      |COMPLETE       |\n",
      "|20043   |2013-12-30 00:00:00|35390      |CLOSED         |\n",
      "|70178   |2013-10-22 00:00:00|35390      |CLOSED         |\n",
      "|107696  |2013-09-07 00:00:00|35390      |COMPLETE       |\n",
      "|282162  |2014-07-07 00:00:00|35390      |PENDING_PAYMENT|\n",
      "+--------+-------------------+-----------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_df.show(5, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = orders_df.filter(\"customer_id = 40322\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------+---------------+\n",
      "|order_id|order_date         |customer_id|order_status   |\n",
      "+--------+-------------------+-----------+---------------+\n",
      "|1       |2014-04-08 00:00:00|40322      |PENDING_PAYMENT|\n",
      "|48390   |2013-09-17 00:00:00|40322      |PENDING_PAYMENT|\n",
      "|87380   |2014-01-08 00:00:00|40322      |PENDING_PAYMENT|\n",
      "|90486   |2013-09-12 00:00:00|40322      |PROCESSING     |\n",
      "|92377   |2013-09-26 00:00:00|40322      |CLOSED         |\n",
      "+--------+-------------------+-----------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_df.show(5,truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PySpark, the `createOrReplaceTempView()` method is used to register a DataFrame as a temporary SQL table (view) so that you can run SQL queries against it. This temporary view will exist only during the lifetime of the Spark session and will be dropped once the session ends or the view is explicitly replaced.\n",
    "\n",
    "**Key Points:**\n",
    "\n",
    "-   **Temporary View:** The view is temporary in the sense that it is only available during the current Spark session.\n",
    "-   **SQL Queries:** Once the DataFrame is registered as a temporary view, you can use spark.sql() to run SQL queries on it, just like any regular SQL table.\n",
    "-   **Replacement:** If a view with the same name already exists, calling createOrReplaceTempView() will replace the old view with the new DataFrame.\n",
    "-   **Global vs. Local:** This creates a local temporary view that is visible only within the current Spark session. If you need a view to be accessible across multiple sessions, you should use createOrReplaceGlobalTempView().\n",
    "\n",
    "**When to Use createOrReplaceTempView:**\n",
    "\n",
    "-   **SQL Queries:** If you prefer to use SQL syntax to query your DataFrame instead of using PySpark DataFrame operations, createOrReplaceTempView() allows you to do that.\n",
    "\n",
    "-   **Intermediate Views:** It's useful for registering intermediate views in a pipeline where multiple transformations happen, and you need to query specific stages using SQL.\n",
    "\n",
    "-   **Replacing Views:** If you want to refresh or modify the structure of the view dynamically, you can replace the previous view with a new one using createOrReplaceTempView().\n",
    "\n",
    "**Important Considerations:**\n",
    "\n",
    "-   **Scope:** The view is only accessible within the current session. If you need a view to persist across sessions, you can use createOrReplaceGlobalTempView().\n",
    "\n",
    "-   **Performance:** Since temporary views are in-memory and don't persist to disk, there is no overhead related to reading from storage. However, complex SQL queries over large DataFrames can still result in performance challenges.\n",
    "\n",
    "**Conclusion:**\n",
    "createOrReplaceTempView() is a powerful method that helps bridge the gap between PySpark DataFrame operations and SQL operations. It makes it easy to query DataFrames using SQL, leveraging the power of Spark SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df.createOrReplaceTempView(\"orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = spark.sql(\"select * from orders where order_status = 'CLOSED'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------+------------+\n",
      "|order_id|         order_date|customer_id|order_status|\n",
      "+--------+-------------------+-----------+------------+\n",
      "|       4|2014-01-04 00:00:00|      34419|      CLOSED|\n",
      "|       7|2014-04-23 00:00:00|       4914|      CLOSED|\n",
      "|       8|2013-10-02 00:00:00|      36928|      CLOSED|\n",
      "|      18|2013-10-06 00:00:00|       1921|      CLOSED|\n",
      "|      25|2014-05-08 00:00:00|      34902|      CLOSED|\n",
      "|      35|2014-03-18 00:00:00|      15715|      CLOSED|\n",
      "|      36|2013-08-16 00:00:00|      10644|      CLOSED|\n",
      "|      45|2013-09-06 00:00:00|      28985|      CLOSED|\n",
      "|      46|2013-10-03 00:00:00|      45110|      CLOSED|\n",
      "|      50|2014-06-05 00:00:00|      35405|      CLOSED|\n",
      "|      55|2014-01-01 00:00:00|      21949|      CLOSED|\n",
      "|      65|2013-07-29 00:00:00|      49936|      CLOSED|\n",
      "|      66|2014-04-22 00:00:00|      19425|      CLOSED|\n",
      "|      73|2013-10-01 00:00:00|      41501|      CLOSED|\n",
      "|      74|2014-05-19 00:00:00|      21753|      CLOSED|\n",
      "|      76|2013-12-17 00:00:00|      25643|      CLOSED|\n",
      "|      78|2014-03-20 00:00:00|       9486|      CLOSED|\n",
      "|      87|2014-03-09 00:00:00|      16164|      CLOSED|\n",
      "|      88|2014-04-03 00:00:00|      13698|      CLOSED|\n",
      "|      92|2013-08-22 00:00:00|      46000|      CLOSED|\n",
      "+--------+-------------------+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df = spark.read.table(\"orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------+---------------+\n",
      "|order_id|         order_date|customer_id|   order_status|\n",
      "+--------+-------------------+-----------+---------------+\n",
      "|       1|2014-04-08 00:00:00|      40322|PENDING_PAYMENT|\n",
      "|       2|2014-05-20 00:00:00|      35390|       COMPLETE|\n",
      "|       3|2013-10-14 00:00:00|      29108|PENDING_PAYMENT|\n",
      "|       4|2014-01-04 00:00:00|      34419|         CLOSED|\n",
      "|       5|2014-02-16 00:00:00|       9936|PENDING_PAYMENT|\n",
      "+--------+-------------------+-----------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/25 13:37:56 WARN ObjectStore: Failed to get database retail, returning NoSuchObjectException\n",
      "24/12/25 13:37:56 WARN ObjectStore: Failed to get database retail, returning NoSuchObjectException\n",
      "24/12/25 13:37:56 WARN ObjectStore: Failed to get database retail, returning NoSuchObjectException\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"create database if not exists retail\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "|   retail|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|   retail|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases\").filter(\"namespace = 'retail'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|   retail|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases\").filter(\"namespace like 'retail%'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----------+\n",
      "|namespace| tableName|isTemporary|\n",
      "+---------+----------+-----------+\n",
      "|   retail|orders_tbl|      false|\n",
      "+---------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"use retail\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----------+\n",
      "|namespace| tableName|isTemporary|\n",
      "+---------+----------+-----------+\n",
      "|   retail|orders_tbl|      false|\n",
      "|         |    orders|       true|\n",
      "+---------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/25 13:39:28 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"create table if not exists orders_tbl (order_id integer, order_date string, customer_id integer, order_status string)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"insert into orders_tbl select * from orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------+---------------+\n",
      "|order_id|         order_date|customer_id|   order_status|\n",
      "+--------+-------------------+-----------+---------------+\n",
      "|       1|2013-07-27 00:00:00|      30265|         CLOSED|\n",
      "|       2|2013-11-25 00:00:00|      20386|         CLOSED|\n",
      "|       3|2014-01-21 00:00:00|      15768|       COMPLETE|\n",
      "|       4|2014-07-04 00:00:00|      27181|     PROCESSING|\n",
      "|       5|2014-03-08 00:00:00|      12448|       COMPLETE|\n",
      "|       6|2014-07-20 00:00:00|      49340|         CLOSED|\n",
      "|       7|2013-12-14 00:00:00|      13801|     PROCESSING|\n",
      "|       8|2014-04-23 00:00:00|      28523|PENDING_PAYMENT|\n",
      "|       9|2014-01-07 00:00:00|      26329|         CLOSED|\n",
      "|      10|2013-07-29 00:00:00|      38797|       COMPLETE|\n",
      "+--------+-------------------+-----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from orders_tbl limit 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+-------+\n",
      "|    col_name|data_type|comment|\n",
      "+------------+---------+-------+\n",
      "|    order_id|      int|   NULL|\n",
      "|  order_date|   string|   NULL|\n",
      "| customer_id|      int|   NULL|\n",
      "|order_status|   string|   NULL|\n",
      "+------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"describe table orders_tbl\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+---------------------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                                  |comment|\n",
      "+----------------------------+---------------------------------------------------------------------------+-------+\n",
      "|order_id                    |int                                                                        |NULL   |\n",
      "|order_date                  |string                                                                     |NULL   |\n",
      "|customer_id                 |int                                                                        |NULL   |\n",
      "|order_status                |string                                                                     |NULL   |\n",
      "|                            |                                                                           |       |\n",
      "|# Detailed Table Information|                                                                           |       |\n",
      "|Catalog                     |spark_catalog                                                              |       |\n",
      "|Database                    |retail                                                                     |       |\n",
      "|Table                       |orders_tbl                                                                 |       |\n",
      "|Owner                       |sugumarsrinivasan                                                          |       |\n",
      "|Created Time                |Wed Dec 25 13:38:30 IST 2024                                               |       |\n",
      "|Last Access                 |UNKNOWN                                                                    |       |\n",
      "|Created By                  |Spark 3.5.3                                                                |       |\n",
      "|Type                        |MANAGED                                                                    |       |\n",
      "|Provider                    |hive                                                                       |       |\n",
      "|Table Properties            |[transient_lastDdlTime=1735114455]                                         |       |\n",
      "|Statistics                  |4242518 bytes                                                              |       |\n",
      "|Location                    |file:/Users/sugumarsrinivasan/Documents/data/warehouse/retail.db/orders_tbl|       |\n",
      "|Serde Library               |org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe                         |       |\n",
      "|InputFormat                 |org.apache.hadoop.mapred.TextInputFormat                                   |       |\n",
      "+----------------------------+---------------------------------------------------------------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"describe extended orders_tbl\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+---------------------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                                  |comment|\n",
      "+----------------------------+---------------------------------------------------------------------------+-------+\n",
      "|order_id                    |int                                                                        |NULL   |\n",
      "|order_date                  |string                                                                     |NULL   |\n",
      "|customer_id                 |int                                                                        |NULL   |\n",
      "|order_status                |string                                                                     |NULL   |\n",
      "|                            |                                                                           |       |\n",
      "|# Detailed Table Information|                                                                           |       |\n",
      "|Catalog                     |spark_catalog                                                              |       |\n",
      "|Database                    |retail                                                                     |       |\n",
      "|Table                       |orders_tbl                                                                 |       |\n",
      "|Owner                       |sugumarsrinivasan                                                          |       |\n",
      "|Created Time                |Wed Dec 25 13:38:30 IST 2024                                               |       |\n",
      "|Last Access                 |UNKNOWN                                                                    |       |\n",
      "|Created By                  |Spark 3.5.3                                                                |       |\n",
      "|Type                        |MANAGED                                                                    |       |\n",
      "|Provider                    |hive                                                                       |       |\n",
      "|Table Properties            |[transient_lastDdlTime=1735114455]                                         |       |\n",
      "|Statistics                  |4242518 bytes                                                              |       |\n",
      "|Location                    |file:/Users/sugumarsrinivasan/Documents/data/warehouse/retail.db/orders_tbl|       |\n",
      "|Serde Library               |org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe                         |       |\n",
      "|InputFormat                 |org.apache.hadoop.mapred.TextInputFormat                                   |       |\n",
      "+----------------------------+---------------------------------------------------------------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"describe formatted orders_tbl\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"drop table orders_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"describe extended orders_tbl\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
