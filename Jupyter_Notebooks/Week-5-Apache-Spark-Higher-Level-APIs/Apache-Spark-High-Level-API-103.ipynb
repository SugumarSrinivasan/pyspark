{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Managed Vs External Table in PySpark:**\n",
    "\n",
    "In PySpark (and Spark SQL in general), the terms managed and external tables refer to how Spark handles the data storage and metadata for those tables in a data warehouse or distributed file system (such as HDFS or S3). Here’s a breakdown of the differences:\n",
    "\n",
    "**1. Managed Table**\n",
    "\n",
    "A managed table (also called an internal table) is a table where Spark manages both the metadata and the data.\n",
    "\n",
    "-   **Storage Location:** Spark stores both the data and metadata in the default location defined by the Spark warehouse. Typically, this is a directory like /user/hive/warehouse in HDFS or a similar path in other storage systems (like S3). You don't need to specify the location explicitly; Spark handles it.\n",
    "\n",
    "-   **Data Management:** Spark owns the lifecycle of both the data and the table. If you drop the table, Spark will delete both the metadata and the data associated with that table.\n",
    "\n",
    "-   **Use Case:** Managed tables are typically used when Spark is responsible for managing the lifecycle of the data and metadata. This is useful when you're working with data that will be used and processed entirely within the Spark ecosystem.\n",
    "\n",
    "**Example of Creating a Managed Table:**\n",
    "\n",
    "```\n",
    "spark.sql(\"CREATE TABLE managed_table (id INT, name STRING) USING parquet\")\n",
    "```\n",
    "\n",
    "If you drop the table:\n",
    "\n",
    "```\n",
    "spark.sql(\"DROP TABLE managed_table\")\n",
    "```\n",
    "\n",
    "This will delete the `table and its data` from the storage location.\n",
    "\n",
    "**2. External Table**\n",
    "\n",
    "An external table refers to a table where Spark manages only the metadata, but not the actual data. The data for an external table resides outside Spark's managed directory, typically in an external storage system like HDFS, S3, or another database.\n",
    "\n",
    "-   **Storage Location:** The data is stored externally (you specify the path explicitly, such as a directory on HDFS or S3). Spark only manages the table metadata, not the actual data files.\n",
    "\n",
    "-   **Data Management:** Spark does not manage the data in external tables. If you drop an external table, only the metadata is removed, not the actual data files.\n",
    "\n",
    "-   **Use Case:** External tables are useful when the data resides in a location managed outside Spark (e.g., HDFS, S3, or a relational database). This allows other systems to access the same data, and Spark can work with it without interfering with the actual data storage.\n",
    "\n",
    "**Example of Creating an External Table:**\n",
    "\n",
    "```\n",
    "spark.sql(\"\"\"\n",
    "CREATE EXTERNAL TABLE external_table (id INT, name STRING)\n",
    "STORED AS parquet\n",
    "LOCATION 's3://my-bucket/external_data/'\n",
    "\"\"\")\n",
    "```\n",
    "\n",
    "If you drop the table:\n",
    "\n",
    "```\n",
    "spark.sql(\"DROP TABLE external_table\")\n",
    "```\n",
    "\n",
    "This will `only delete the metadata`, and the data files in s3://my-bucket/external_data/ will remain intact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary of Key Differences**\n",
    "\n",
    "\n",
    "| Feature                  | Managed Table                          | External Table                       |\n",
    "|--------------------------|----------------------------------------|--------------------------------------|\n",
    "| **Data Management**       | Spark manages both data and metadata.  | Spark manages metadata only. Data remains in the specified external location. |\n",
    "| **Storage Location**      | Spark defines and manages the storage location. Typically in the default warehouse directory. | You define the storage location (e.g., HDFS, S3). |\n",
    "| **Data Deletion on Drop** | Both data and metadata are deleted.    | Only metadata is deleted; data remains. |\n",
    "| **Use Case**              | When Spark is responsible for both the data and metadata management. | When the data is managed externally and Spark only needs to manage the metadata. |\n",
    "\n",
    "**When to Use Which:**\n",
    "\n",
    "-   **Managed Tables:** Use them when you want Spark to have full control over both the table metadata and the data. This is ideal for temporary or intermediate data that you don’t want to manage outside Spark.\n",
    "\n",
    "-   **External Tables:** Use them when you want to reference data that already exists outside Spark’s control, such as in HDFS, S3, or another system, and you don't want Spark to delete the actual data when dropping the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/25 19:24:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init\n",
    "import getpass\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "username = getpass.getuser()\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config(\"spark.sql.catalogImplementation\", \"hive\"). \\\n",
    "    config(\"spark.sql.warehouse.dir\",f\"/Users/{username}/Documents/data/warehouse\"). \\\n",
    "    enableHiveSupport(). \\\n",
    "    master(\"local\"). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/25 19:24:24 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "24/12/25 19:24:24 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "24/12/25 19:24:25 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "24/12/25 19:24:25 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore sugumarsrinivasan@192.168.31.195\n",
      "24/12/25 19:24:25 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"use retail\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/25 19:26:56 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider csv. Persisting data source table `spark_catalog`.`retail`.`orders_ext` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n",
      "24/12/25 19:26:56 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "24/12/25 19:26:56 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "24/12/25 19:26:56 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "24/12/25 19:26:56 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"create table if not exists orders_ext (order_id integer, order_date string, customer_id integer, order_status string) using csv location '/Users/sugumarsrinivasan/Documents/data/orders'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----------+\n",
      "|namespace| tableName|isTemporary|\n",
      "+---------+----------+-----------+\n",
      "|   retail|orders_ext|      false|\n",
      "+---------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------------+-----------+------------+\n",
      "|order_id|order_date           |customer_id|order_status|\n",
      "+--------+---------------------+-----------+------------+\n",
      "|1       |2013-07-27 00:00:00.0|30265      |CLOSED      |\n",
      "|2       |2013-11-25 00:00:00.0|20386      |CLOSED      |\n",
      "|3       |2014-01-21 00:00:00.0|15768      |COMPLETE    |\n",
      "|4       |2014-07-04 00:00:00.0|27181      |PROCESSING  |\n",
      "|5       |2014-03-08 00:00:00.0|12448      |COMPLETE    |\n",
      "+--------+---------------------+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from spark_catalog.retail.orders_ext limit 5\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+---------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                |comment|\n",
      "+----------------------------+---------------------------------------------------------+-------+\n",
      "|order_id                    |int                                                      |NULL   |\n",
      "|order_date                  |string                                                   |NULL   |\n",
      "|customer_id                 |int                                                      |NULL   |\n",
      "|order_status                |string                                                   |NULL   |\n",
      "|                            |                                                         |       |\n",
      "|# Detailed Table Information|                                                         |       |\n",
      "|Catalog                     |spark_catalog                                            |       |\n",
      "|Database                    |retail                                                   |       |\n",
      "|Table                       |orders_ext                                               |       |\n",
      "|Owner                       |sugumarsrinivasan                                        |       |\n",
      "|Created Time                |Wed Dec 25 19:26:56 IST 2024                             |       |\n",
      "|Last Access                 |UNKNOWN                                                  |       |\n",
      "|Created By                  |Spark 3.5.3                                              |       |\n",
      "|Type                        |EXTERNAL                                                 |       |\n",
      "|Provider                    |csv                                                      |       |\n",
      "|Location                    |file:/Users/sugumarsrinivasan/Documents/data/orders      |       |\n",
      "|Serde Library               |org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe       |       |\n",
      "|InputFormat                 |org.apache.hadoop.mapred.SequenceFileInputFormat         |       |\n",
      "|OutputFormat                |org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat|       |\n",
      "+----------------------------+---------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"describe formatted orders_ext\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"truncate table spark_catalog.retail.orders_ext\")\n",
    "\n",
    "# output:\n",
    "# AnalysisException: Operation not allowed: TRUNCATE TABLE on external tables: `spark_catalog`.`retail`.`orders_ext`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Manipulation Language (DML) operations like INSERT, UPDATE, DELETE, and SELECT can be performed on external tables in PySpark, but there are some important distinctions and limitations to be aware of. Here's a breakdown:\n",
    "\n",
    "**1. SELECT**\n",
    "\n",
    "-   **Works on External Tables:** The SELECT operation works as expected on external tables because it queries the data stored externally (e.g., in HDFS, S3, etc.) based on the table schema (metadata).\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```\n",
    "spark.sql(\"SELECT * FROM external_table\")\n",
    "```\n",
    "\n",
    "**2. INSERT**\n",
    "\n",
    "-   **Works on External Tables:** You can perform INSERT operations to add data into an external table. This will write data into the location defined for the external table (e.g., a directory in HDFS or S3).\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```\n",
    "spark.sql(\"INSERT INTO external_table VALUES (1, 'John')\")\n",
    "```\n",
    "\n",
    "However, keep in mind that Spark will append the data to the external location, so it doesn't manage partitions or data consistency the same way as managed tables.\n",
    "\n",
    "**3. UPDATE**\n",
    "\n",
    "-   **Limited Support for External Tables:**\n",
    "\n",
    "    -   **Not directly supported in Spark:** Spark does not support UPDATE operations natively on external tables because Spark writes data in a distributed and immutable way. The data in external storage systems like HDFS or S3 is not updated in-place; instead, Spark typically rewrites data during operations like INSERT or MERGE.\n",
    "    -   For UPDATE-like functionality, you can use Delta Lake (if using Delta format) or use a workaround with overwriting data.\n",
    "\n",
    "**Workaround using DataFrame operations (not ideal):**\n",
    "\n",
    "```\n",
    "df = spark.read.parquet(\"s3://my-bucket/external_table/\")\n",
    "\n",
    "# Perform transformations (e.g., updating a value)\n",
    "updated_df = df.withColumn(\"name\", F.when(df.id == 1, \"Updated Name\").otherwise(df.name))\n",
    "\n",
    "# Overwrite the external table data\n",
    "updated_df.write.mode(\"overwrite\").parquet(\"s3://my-bucket/external_table/\")\n",
    "```\n",
    "\n",
    "-   **Delta Lake Support:** If you are using Delta Lake, the UPDATE operation is supported on external tables as it provides ACID transactions.\n",
    "\n",
    "**Example with Delta:**\n",
    "\n",
    "```\n",
    "delta_table = DeltaTable.forPath(spark, \"s3://my-bucket/external_table/\")\n",
    "delta_table.update(condition = \"id = 1\", set = {\"name\": \"'Updated Name'\"})\n",
    "```\n",
    "\n",
    "**4. DELETE**\n",
    "\n",
    "-   **Limited Support for External Tables:**\n",
    "    -   Like UPDATE, the DELETE operation is not natively supported in Spark on external tables because Spark doesn't handle row-level deletions in the same way a traditional RDBMS might.\n",
    "    -   Workaround using DataFrame operations: You can filter out rows and rewrite the data (similar to how UPDATE is handled).\n",
    "\n",
    "**Workaround using DataFrame operations:**\n",
    "```\n",
    "df = spark.read.parquet(\"s3://my-bucket/external_table/\")\n",
    "\n",
    "# Filter out rows you want to delete\n",
    "df_filtered = df.filter(df.id != 1)\n",
    "\n",
    "# Overwrite the external table data with the remaining rows\n",
    "df_filtered.write.mode(\"overwrite\").parquet(\"s3://my-bucket/external_table/\")\n",
    "```\n",
    "\n",
    "**Delta Lake Support:** Delta Lake supports DELETE operations on external tables with ACID transactions.\n",
    "\n",
    "**Example with Delta:**\n",
    "\n",
    "```\n",
    "delta_table = DeltaTable.forPath(spark, \"s3://my-bucket/external_table/\")\n",
    "delta_table.delete(\"id = 1\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary of DML Operations on External Tables**\n",
    "\n",
    "| DML Operation | Managed Tables | External Tables (Non-Delta) | External Tables (Delta Lake) |\n",
    "|---------------|----------------|-----------------------------|------------------------------|\n",
    "| **SELECT**    | Yes            | Yes                         | Yes                          |\n",
    "| **INSERT**    | Yes            | Yes                         | Yes                          |\n",
    "| **UPDATE**    | Yes            | No (unless using overwrite) | Yes (with Delta Lake)        |\n",
    "| **DELETE**    | Yes            | No (unless using overwrite) | Yes (with Delta Lake)        |\n",
    "\n",
    "\n",
    "**Key Points:**\n",
    "\n",
    "-   SELECT and INSERT are fully supported on external tables.\n",
    "\n",
    "-   UPDATE and DELETE are not natively supported on external tables unless using Delta Lake for ACID transactions.\n",
    "\n",
    "-   For non-Delta external tables, you can workaround UPDATE and DELETE by reading the data, modifying it, and overwriting the external location.\n",
    "\n",
    "**Recommendation:**\n",
    "\n",
    "-   If you need advanced support for DML operations (especially UPDATE and DELETE), consider using Delta Lake (which provides ACID transactions and better handling of these operations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"insert into spark_catalog.retail.orders_ext values(105, '2013-07-27 00:00:00.0', 5555, 'CLOSED')\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
