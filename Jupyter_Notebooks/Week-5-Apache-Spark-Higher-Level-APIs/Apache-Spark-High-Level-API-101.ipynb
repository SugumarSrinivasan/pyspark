{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DataFrame in PySpark:**\n",
    "\n",
    "A DataFrame in PySpark is a distributed collection of data organized into named columns, and it's similar to a table in a relational database or a data frame in Pandas. The core idea behind a DataFrame is to provide a higher-level abstraction over RDDs (Resilient Distributed Datasets), which allows for more expressive operations (like SQL-like queries) while maintaining scalability and performance.\n",
    "\n",
    "Here are some key features of a PySpark DataFrame:\n",
    "\n",
    "-   **Schema-based:** DataFrames have a schema, which means the data is structured with columns that have types, similar to a relational database table.\n",
    "\n",
    "-   **Distributed:** DataFrames are distributed across a cluster, and Spark will automatically manage parallelization and fault tolerance.\n",
    "\n",
    "-   **Optimized Execution:** PySpark DataFrames use Spark’s Catalyst query optimizer to optimize query execution plans.\n",
    "\n",
    "-   **Interoperability:** DataFrames can be created from various sources like CSV files, Parquet files, databases, and other distributed file systems (HDFS, S3, etc.).\n",
    "\n",
    "-   **High-level API:** The API is more user-friendly and higher-level than working directly with RDDs.\n",
    "\n",
    "**Example of creating a DataFrame:**\n",
    "\n",
    "```\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"Example\").getOrCreate()\n",
    "\n",
    "# Create a DataFrame from a list of tuples\n",
    "data = [(\"Alice\", 29), (\"Bob\", 30), (\"Charlie\", 35)]\n",
    "columns = [\"Name\", \"Age\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "df.show()\n",
    "```\n",
    "\n",
    "**Example Output:**\n",
    "```\n",
    "+-------+---+\n",
    "|   Name|Age|\n",
    "+-------+---+\n",
    "|  Alice| 29|\n",
    "|    Bob| 30|\n",
    "|Charlie| 35|\n",
    "+-------+---+\n",
    "```\n",
    "\n",
    "**Spark SQL:**\n",
    "\n",
    "Spark SQL is a Spark module for structured data processing. It allows you to run SQL queries on data stored in a variety of formats, including DataFrames, tables, and external sources like Hive or relational databases.\n",
    "\n",
    "Spark SQL integrates relational data processing with Spark’s functional programming API, allowing users to execute SQL queries along with transformations and actions on DataFrames and Datasets.\n",
    "\n",
    "Key features of Spark SQL:\n",
    "\n",
    "-   **SQL Queries:** You can run standard SQL queries on data in DataFrames or external sources (like Hive).\n",
    "Unified Data Access: You can use the same DataFrame API or Spark SQL to query different data sources (e.g., HDFS, S3, Parquet, JDBC).\n",
    "\n",
    "-   **Catalyst Optimizer:** Spark SQL uses the Catalyst optimizer to optimize query execution for better performance.\n",
    "\n",
    "**Example of running SQL queries with Spark SQL:**\n",
    "\n",
    "**1.Register a DataFrame as a temporary view:**\n",
    "\n",
    "```\n",
    "# Create a DataFrame\n",
    "data = [(\"Alice\", 29), (\"Bob\", 30), (\"Charlie\", 35)]\n",
    "columns = [\"Name\", \"Age\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Register the DataFrame as a temporary SQL view\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "```\n",
    "\n",
    "**2.Run a SQL query:**\n",
    "\n",
    "```\n",
    "# Execute SQL query\n",
    "sql_df = spark.sql(\"SELECT * FROM people WHERE Age > 30\")\n",
    "\n",
    "# Show results\n",
    "sql_df.show()\n",
    "```\n",
    "\n",
    "**Example Output:**\n",
    "\n",
    "```\n",
    "+-------+---+\n",
    "|   Name|Age|\n",
    "+-------+---+\n",
    "|Charlie| 35|\n",
    "+-------+---+\n",
    "```\n",
    "\n",
    "**Key Differences:**\n",
    "\n",
    "-   **DataFrame API:** Provides a programmatic way to work with data using functions like filter(), select(), groupBy(), etc.\n",
    "\n",
    "-   **Spark SQL:** Allows you to express queries in SQL syntax and can be used in combination with the DataFrame API.\n",
    "\n",
    "\n",
    "\n",
    "| Feature                  | Spark SQL (SQL API)                                    | PySpark (Python API)                                    |\n",
    "|--------------------------|--------------------------------------------------------|---------------------------------------------------------|\n",
    "| **Definition**            | A DataFrame in Spark SQL is a distributed collection of data organized into named columns, similar to a table in a relational database. | A DataFrame in PySpark is a distributed collection of data organized into named columns, similar to a table in a relational database, but accessed via Python code. |\n",
    "| **Language**              | SQL-based interface for querying data.                | Python-based interface for querying data using PySpark functions. |\n",
    "| **Creation**              | Typically created using `CREATE TEMPORARY VIEW` or directly reading from external data sources with `spark.read` in SQL queries. | Created using PySpark's `spark.read` or loading external data, and can be manipulated using PySpark’s Python functions. |\n",
    "| **Querying**              | Queries are executed using SQL syntax directly on DataFrames. | Queries are executed using PySpark’s DataFrame API (e.g., `filter()`, `select()`, `groupBy()`) or SQL queries via `spark.sql()`. |\n",
    "| **Operations**            | Operations are written as SQL queries, like `SELECT`, `JOIN`, `GROUP BY`, `ORDER BY`, etc. | Operations are written using PySpark DataFrame API methods, like `.select()`, `.filter()`, `.join()`, `.agg()`, etc. |\n",
    "| **Interactivity**         | You interact with DataFrames in Spark SQL by running SQL queries through the `spark.sql()` interface. | You interact with DataFrames in PySpark using Python code and PySpark API functions. |\n",
    "| **Performance**           | Spark SQL queries are optimized via Catalyst optimizer. | PySpark queries also use Catalyst optimizer, but operations are written in Python, which may involve extra overhead. |\n",
    "| **Syntax**                | Uses SQL syntax, e.g., `SELECT * FROM df WHERE age > 30`. | Uses Python syntax with PySpark functions, e.g., `df.filter(df.age > 30)`. |\n",
    "| **Integration with Other Libraries** | SQL queries can be easily integrated with other Spark components and tools like Spark MLlib, GraphX, etc. | PySpark DataFrame API integrates easily with other Python libraries like Pandas, NumPy, and SciPy for further analysis. |\n",
    "| **Compatibility**         | Best suited for users familiar with SQL or databases.   | Ideal for users comfortable with Python and the PySpark API. |\n",
    "| **Execution Environment** | Runs inside a Spark session, typically via `spark.sql()`. | Runs inside a PySpark session using Python's interactive shell or scripts. |\n",
    "\n",
    "\n",
    "**When to use which?**\n",
    "\n",
    "-   **Use DataFrame API** when you want to stay within the functional programming paradigm and benefit from optimization and transformations on distributed data.\n",
    "\n",
    "-   **Use Spark SQL** if you're more comfortable with SQL or if you're working in a scenario where SQL queries are a more natural fit, especially when querying data sources like Hive or JDBC.\n",
    "\n",
    "Both DataFrame API and Spark SQL are optimized by the Catalyst Optimizer, so performance-wise, there is little difference between the two. It largely depends on the preference of the user (SQL vs. programmatic interface)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/25 09:07:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/12/25 09:07:41 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init\n",
    "import getpass\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "username = getpass.getuser()\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config(\"spark.sql.warehouse.dir\",f\"/Users/{username}/Documents/data/warehouse\"). \\\n",
    "    master(\"local\"). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**read():** Used for read the data from files and load it to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df = spark.read \\\n",
    ".format(\"csv\") \\\n",
    ".option(\"header\", \"true\") \\\n",
    ".option(\"inferSchema\", \"true\") \\\n",
    ".load(\"/Users/sugumarsrinivasan/Documents/data/sample_orders_1GB.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**show():**\n",
    "\n",
    "-   It is used for printing the contents from dataframe.\n",
    "-   show(5) --> It prints the first 5 rows from dataframe.\n",
    "-   show() --> It prints the first 20 rows if you don't pass any numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------+---------------+\n",
      "|order_id|         order_date|customer_id|   order_status|\n",
      "+--------+-------------------+-----------+---------------+\n",
      "|       1|2014-04-08 00:00:00|      40322|PENDING_PAYMENT|\n",
      "|       2|2014-05-20 00:00:00|      35390|       COMPLETE|\n",
      "|       3|2013-10-14 00:00:00|      29108|PENDING_PAYMENT|\n",
      "|       4|2014-01-04 00:00:00|      34419|         CLOSED|\n",
      "|       5|2014-02-16 00:00:00|       9936|PENDING_PAYMENT|\n",
      "|       6|2014-05-02 00:00:00|      41598|       COMPLETE|\n",
      "|       7|2014-04-23 00:00:00|       4914|         CLOSED|\n",
      "|       8|2013-10-02 00:00:00|      36928|         CLOSED|\n",
      "|       9|2013-11-29 00:00:00|      17318|     PROCESSING|\n",
      "|      10|2013-10-09 00:00:00|      46757|     PROCESSING|\n",
      "|      11|2013-11-16 00:00:00|      34795|PENDING_PAYMENT|\n",
      "|      12|2013-12-18 00:00:00|      34931|PENDING_PAYMENT|\n",
      "|      13|2014-02-01 00:00:00|      32229|       COMPLETE|\n",
      "|      14|2014-05-10 00:00:00|       1438|     PROCESSING|\n",
      "|      15|2014-01-20 00:00:00|      32629|     PROCESSING|\n",
      "|      16|2013-12-04 00:00:00|      42328|       COMPLETE|\n",
      "|      17|2014-05-04 00:00:00|      25221|     PROCESSING|\n",
      "|      18|2013-10-06 00:00:00|       1921|         CLOSED|\n",
      "|      19|2014-06-20 00:00:00|      20424|PENDING_PAYMENT|\n",
      "|      20|2013-08-29 00:00:00|      11502|PENDING_PAYMENT|\n",
      "+--------+-------------------+-----------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------+---------------+\n",
      "|order_id|         order_date|customer_id|   order_status|\n",
      "+--------+-------------------+-----------+---------------+\n",
      "|       1|2014-04-08 00:00:00|      40322|PENDING_PAYMENT|\n",
      "|       2|2014-05-20 00:00:00|      35390|       COMPLETE|\n",
      "|       3|2013-10-14 00:00:00|      29108|PENDING_PAYMENT|\n",
      "|       4|2014-01-04 00:00:00|      34419|         CLOSED|\n",
      "|       5|2014-02-16 00:00:00|       9936|PENDING_PAYMENT|\n",
      "+--------+-------------------+-----------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**printSchema():** Used to print the schema of a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: timestamp (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**withColumnRenamed():** Used to rename the existing columns names in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df1 = orders_df.withColumnRenamed(\"order_status\", \"status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------+---------------+\n",
      "|order_id|         order_date|customer_id|         status|\n",
      "+--------+-------------------+-----------+---------------+\n",
      "|       1|2014-04-08 00:00:00|      40322|PENDING_PAYMENT|\n",
      "|       2|2014-05-20 00:00:00|      35390|       COMPLETE|\n",
      "|       3|2013-10-14 00:00:00|      29108|PENDING_PAYMENT|\n",
      "|       4|2014-01-04 00:00:00|      34419|         CLOSED|\n",
      "|       5|2014-02-16 00:00:00|       9936|PENDING_PAYMENT|\n",
      "|       6|2014-05-02 00:00:00|      41598|       COMPLETE|\n",
      "|       7|2014-04-23 00:00:00|       4914|         CLOSED|\n",
      "|       8|2013-10-02 00:00:00|      36928|         CLOSED|\n",
      "|       9|2013-11-29 00:00:00|      17318|     PROCESSING|\n",
      "|      10|2013-10-09 00:00:00|      46757|     PROCESSING|\n",
      "|      11|2013-11-16 00:00:00|      34795|PENDING_PAYMENT|\n",
      "|      12|2013-12-18 00:00:00|      34931|PENDING_PAYMENT|\n",
      "|      13|2014-02-01 00:00:00|      32229|       COMPLETE|\n",
      "|      14|2014-05-10 00:00:00|       1438|     PROCESSING|\n",
      "|      15|2014-01-20 00:00:00|      32629|     PROCESSING|\n",
      "|      16|2013-12-04 00:00:00|      42328|       COMPLETE|\n",
      "|      17|2014-05-04 00:00:00|      25221|     PROCESSING|\n",
      "|      18|2013-10-06 00:00:00|       1921|         CLOSED|\n",
      "|      19|2014-06-20 00:00:00|      20424|PENDING_PAYMENT|\n",
      "|      20|2013-08-29 00:00:00|      11502|PENDING_PAYMENT|\n",
      "+--------+-------------------+-----------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed_df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   **withColumn():** Used to modify the existing columns or Add a new column in the dataframe.\n",
    "-   **to_timestamp():** Used to converting the data type from string to timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp\n",
    "transformed_df2 = transformed_df1.withColumn(\"orders_date_new\", to_timestamp(\"order_date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: timestamp (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- orders_date_new: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed_df2.printSchema()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
