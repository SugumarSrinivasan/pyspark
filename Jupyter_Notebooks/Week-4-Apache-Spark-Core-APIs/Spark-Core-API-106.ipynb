{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BroadCast Join:**\n",
    "\n",
    "In Apache Spark, a broadcast join is an optimization technique used to perform joins more efficiently, especially when one of the DataFrames (or RDDs) is significantly smaller than the other. In a broadcast join, Spark \"broadcasts\" the smaller dataset to all the worker nodes in the cluster so that the larger dataset can be joined locally with the smaller dataset on each worker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Concepts:**\n",
    "-   Broadcasting means sending a copy of the smaller DataFrame (or RDD) to each executor, avoiding the need to shuffle data across the network.\n",
    "-   Shuffle is typically required in join operations when the data needs to be exchanged between different executors based on the join key. Broadcast join bypasses this step.\n",
    "-   Broadcasting is most effective when one of the datasets is small enough to fit in memory on each executor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/20 19:15:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySparkExample\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_base = spark.sparkContext.textFile(\"/Users/sugumarsrinivasan/Documents/data/orders.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "orders_header = orders_base.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_data_without_header = orders_base.filter(lambda line: line != orders_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_mapped = orders_data_without_header.map(lambda x: (x.split(\",\")[2], x.split(\",\")[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_base = spark.sparkContext.textFile(\"/Users/sugumarsrinivasan/Documents/data/customers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_header = customers_base.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_data_without_header = customers_base.filter(lambda line: line != customers_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_mapped = customers_data_without_header.map(lambda x: (x.split(\",\")[0], x.split(\",\")[8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_broadcast = spark.sparkContext.broadcast(customers_mapped.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pincode(customer_id):\n",
    "    try:\n",
    "        return customers_broadcast.value[customer_id]\n",
    "    except:\n",
    "        return \"-1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_rdd = orders_mapped.map(lambda x: (get_pincode(int(x[0])),x[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_rdd.saveAsTextFile(\"/Users/sugumarsrinivasan/Documents/data/broadcastresult\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Details of spark UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local Image](./screenshots/spark-broadcast-job.png)\n",
    "![Local Image](./screenshots/spark-broadcast-stage.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
