{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Difference Between reduceByKey() and reduce() in Pyspark:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Feature             | `reduceByKey`                                          | `reduce`                                         |\n",
    "|---------------------|--------------------------------------------------------|--------------------------------------------------|\n",
    "| **Purpose**          | Aggregates values of the same key.                     | Aggregates all elements of the RDD into a single result. |\n",
    "| **Input Type**       | Works on an RDD of key-value pairs (e.g., `(key, value)`). | Works on an RDD of any type (not necessarily key-value pairs). |\n",
    "| **Function**         | Takes a function that combines two values of the same key (e.g., `lambda x, y: x + y`). | Takes a binary function that reduces two values of the same type into one (e.g., `lambda x, y: x + y`). |\n",
    "| **Shuffling**        | Causes **shuffling** of data across partitions because of key-based grouping. | No shuffling; operates on the entire dataset. |\n",
    "| **Return Type**      | Returns a new RDD of key-value pairs with reduced values for each key. | Returns a single result (aggregated value). |\n",
    "| **Use Case**         | Used when you need to aggregate values by key (e.g., summing values for each key). | Used when you want to aggregate all the elements of the RDD into a single value (e.g., sum, max). |\n",
    "| **Execution**        | Operates in multiple steps: first locally on each partition and then across partitions. | Performs the reduction operation in a single pass across the entire dataset. |\n",
    "| **Example**          | `rdd.reduceByKey(lambda x, y: x + y)` (e.g., summing values by key) | `rdd.reduce(lambda x, y: x + y)` (e.g., summing all values) |\n",
    "| **Performance**      | Generally more efficient for key-based aggregations due to parallelism. | Can be slower for large datasets, as it requires a full pass through the data. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanations of Differences:**\n",
    "\n",
    "*   **reduceByKey:**\n",
    "    -   Specifically designed for aggregating values based on a key (used on RDDs of key-value pairs).\n",
    "    -   The operation is parallel and distributed across partitions, with shuffling occurring when data needs to be grouped by key.\n",
    "    -   The result is an RDD where each key maps to a reduced value.\n",
    "\n",
    "*   **reduce:**\n",
    "    -   Works on all elements of the RDD, reducing them to a single value.\n",
    "    -   The reduction is done sequentially, and it doesn't rely on keys or groupings.\n",
    "    -   No shuffling is involved since there is no concept of key-based grouping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`reduceByKey()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySparkExample\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = spark.sparkContext.parallelize([('a', 1), ('b', 2), ('a', 3), ('b', 4)])\n",
    "result = rdd.reduceByKey(lambda x, y: x + y)\n",
    "result.collect()\n",
    "# Output: [('a', 4), ('b', 6)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`reduce()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5])\n",
    "result = rdd.reduce(lambda x, y: x + y)\n",
    "print(result)\n",
    "# Output: 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In summary:**\n",
    "\n",
    "*   Use `reduceByKey` when you're working with key-value pairs and need to aggregate by key.\n",
    "*   Use `reduce` when you want to aggregate the entire dataset into a single value, regardless of keys."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
