{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transformations in Apache Spark:**\n",
    "\n",
    "A transformation refers to an operation that produces a new RDD (Resilient Distributed Dataset) or DataFrame from an existing one. Transformations are lazy operations, meaning that Spark doesn't immediately execute the transformation when it is called. Instead, Spark builds an execution plan and only executes the transformations when an action is performed on the RDD or DataFrame.\n",
    "\n",
    "Transformations allow you to define the computation steps on your data, such as filtering, mapping, reducing, joining, etc.\n",
    "\n",
    "**Key Characteristics of Transformations in Spark:**\n",
    "\n",
    "*   **Lazy Execution:** Transformations do not execute immediately. Instead, Spark constructs a DAG (Directed Acyclic Graph) that represents the lineage of transformations applied on the data. The actual computation is triggered only when an action is called (e.g., collect(), save(), count()).\n",
    "\n",
    "* **Immutable:** Each transformation produces a new RDD or DataFrame; the original dataset remains unchanged.\n",
    "\n",
    "* **Transformations can be Narrow or Wide:** Based on how data is shuffled across partitions, transformations are categorized into narrow and wide transformations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Narrow Transformation:**\n",
    "\n",
    "In **PySpark**, a Narrow Transformation refers to a type of transformation where each input partition contributes to at most one output partition. This means that during a narrow transformation, data is not shuffled across the cluster, and each element in the input RDD (Resilient Distributed Dataset) or DataFrame is mapped to a single corresponding element in the output.\n",
    "\n",
    "**Key Characteristics of Narrow Transformations:**\n",
    "\n",
    "* **No Data Shuffling:** Data does not need to be moved between different nodes in the cluster. This makes narrow transformations more efficient than wide transformations, which involve data shuffling.\n",
    "* **One-to-One Relationship:** Each element of the input is processed independently, with a one-to-one mapping between input and output partitions.\n",
    "* **Local Computation:** Operations are carried out locally within each partition without needing data from other partitions.\n",
    "\n",
    "**Examples of Narrow Transformations in PySpark:**\n",
    "\n",
    "-   **map():** Transforms each element of the RDD or DataFrame by applying a function.\n",
    "-   **filter():** Filters the data by applying a condition and returns the elements that satisfy it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySparkExample\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5])\n",
    "result_rdd = rdd.map(lambda x: x * 2) # There is no shuffle\n",
    "print(result_rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the `map()` operation applies a function to each element, but all computations are done within each partition without any need for shuffling the data between nodes.\n",
    "\n",
    "**Benefits:**\n",
    "\n",
    "*   **Performance:** Narrow transformations are typically faster because they avoid expensive data shuffling.\n",
    "*   **Less Network I/O:** Since thereâ€™s no data movement between partitions, the operations are more efficient and less taxing on network resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wide Transformation:**\n",
    "\n",
    "A wide transformation refers to a type of operation where data from one partition can end up being sent to multiple partitions. These transformations require shuffling of data between different nodes in the cluster, as data needs to be reorganized for the computation.\n",
    "\n",
    "**Key Characteristics of Wide Transformations:**\n",
    "\n",
    "*   **Shuffle operation:** Wide transformations cause a shuffle, meaning that Spark has to redistribute the data across different partitions.\n",
    "*   **More expensive:** Since shuffling involves disk and network I/O, wide transformations can be costly in terms of time and resources.\n",
    "*   **Group and aggregate data:** Wide transformations typically involve operations where data from multiple partitions needs to be grouped, aggregated, or redistributed based on some key.\n",
    "\n",
    "**Examples of Wide Transformations in pyspark:**\n",
    "\n",
    "*   **groupByKey():** Groups the data based on the key, requiring a shuffle to reorganize data.\n",
    "*   **reduceByKey():** Reduces the data by key and requires shuffling because data with the same key needs to be brought together.\n",
    "*   **join():** When you join two datasets (e.g., RDDs or DataFrames) based on a key, Spark needs to shuffle the data to match the keys across partitions.\n",
    "\n",
    "\n",
    "**Why Are Wide Transformations Costly?**\n",
    "\n",
    "*   **Data Movement:** Wide transformations require moving data across the cluster. For example, in a groupByKey() operation, all the values with the same key must be co-located, potentially requiring the movement of data across multiple nodes.\n",
    "*   **Disk I/O:** If there is not enough memory to perform the transformation, Spark might spill data to disk during the shuffle process.\n",
    "*   **Network I/O:** The data shuffle can also result in significant network overhead, especially in large clusters or when working with large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = spark.sparkContext.parallelize([('a', 1), ('b', 2), ('a', 3)])\n",
    "reduced = rdd.reduceByKey(lambda x, y: x + y)  # This involves a shuffle\n",
    "reduced.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary:**\n",
    "\n",
    "Wide transformations require a shuffle and typically involve operations that redistribute data across partitions based on some key, such as `groupByKey()`, `reduceByKey()`, `join()`, and `distinct()`.\n",
    "These operations are more expensive due to the need for data movement, network I/O, and possible disk spills."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   **Spark History Server:**\n",
    "\n",
    "    -   A web UI to monitor and visualize completed jobs, stages, and tasks.\n",
    "    -   The web UI is accessible through the port 18080.\n",
    "*   **Job:**\n",
    "\n",
    "    -   A complete unit of work initiated by an action, consisting of one or more stages.\n",
    "    -   Number of Jobs created = Number of Actions being called.\n",
    "*   **Stage:**\n",
    "    -   A subset of a job, divided by wide transformations. Stages consist of multiple tasks.\n",
    "    -   Number of Stages = Number of Wide Transformation Used + 1\n",
    "*   **Tasks:** \n",
    "    -   The smallest unit of execution in Spark, operating on a partition of the data.\n",
    "    -   Number of Tasks = Number of Partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Narrow Transformation Spark UI:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![local image](./screenshots/narrow-trans-job.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![local Image](./screenshots/narrow-trans-stages-tasks.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wide Transformation Spark UI:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![local image](./screenshots/wide-trans-job.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local Image](./screenshots/wide-trans-stages-tasks.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pyspark-env)",
   "language": "python",
   "name": "pyspark-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
