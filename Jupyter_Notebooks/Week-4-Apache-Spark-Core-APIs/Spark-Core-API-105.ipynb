{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Join:**\n",
    "\n",
    "In Apache Spark, a join transformation refers to an operation that combines two or more datasets (typically two DataFrames or RDDs) based on a common key or condition, producing a new dataset that includes columns from both input datasets. The resulting dataset usually contains rows where the join condition holds true.\n",
    "\n",
    "Spark provides several types of join operations, and these can be applied to both DataFrames and RDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/20 17:46:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySparkExample\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_base = spark.sparkContext.textFile(\"/Users/sugumarsrinivasan/Documents/data/orders.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_mapped = orders_base.map(lambda x: (x.split(\",\")[2], x.split(\",\")[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_base = spark.sparkContext.textFile(\"/Users/sugumarsrinivasan/Documents/data/customers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_mapped = customers_base.map(lambda x: (x.split(\",\")[0], x.split(\",\")[8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_rdd = customers_mapped.join(orders_mapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "joined_rdd.saveAsTextFile(\"/Users/sugumarsrinivasan/Documents/data/output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Application details in spark UI:\n",
    "\n",
    "-   Used the `saveAsTextFile()` action and hence the job is 1.\n",
    "-   Used `join()` transformation which is wide, that's why you see 2 stages.\n",
    "-   Since both the datasets(customers.csv and orders.csv) are less than the block size, that's why tasks are 2 for each dataset(`spark.sparkContext.defaultMinPartitions`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local Image](/Users/sugumarsrinivasan/Developer/Coding/pyspark/Jupyter_Notebooks/screenshots/spark-trans-join-job.png)\n",
    "![Local image](/Users/sugumarsrinivasan/Developer/Coding/pyspark/Jupyter_Notebooks/screenshots/spark-trans-join.png)\n",
    "![Local Image](/Users/sugumarsrinivasan/Developer/Coding/pyspark/Jupyter_Notebooks/screenshots/spark-trans-join-stg.png)\n",
    "![Local Image](/Users/sugumarsrinivasan/Developer/Coding/pyspark/Jupyter_Notebooks/screenshots/spark-trans-join-agg.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pyspark-env)",
   "language": "python",
   "name": "pyspark-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
